---
title: "Projects"
format:
  html:
    embed-resources: true
    allow-raw-html: true
    toc: false
---

### Ô∏èüìè Job Satisfaction Measurement using LLM-embeddings

<p>[Date:]{style="font-weight: bold;"} January 16, 2025</p>

Traditional Likert scales, the preferred choice for many psychology researchers, come with notable limitations: predefined response options that constrain responses, careless responding, debates about whether response categories are truly equidistant, and susceptibility to common method bias, which can inflate relationships. In contrast, natural language, expressed through open text, is how we naturally communicate. What if we could leverage this to measure psychological constructs?

In my master‚Äôs thesis, I explore the use of BERT-based LLM embeddings to quantify text responses and measure job attitudes, with a focus on job satisfaction. I evaluate the effectiveness of these embeddings by examining both their construct validity and criterion validity in assessing job satisfaction.

::: {layout-ncol="2"}
::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="ma_thesis/RQ1_final_plot.png" alt="Correlation between LLM-predicted and actual job satisfaction" class="zoom-image" style="cursor: pointer; display: block; margin: 0 auto; max-width: auto; height: 300px;"/>

<br>

<p style="font-weight: bold;">

*Figure 1*: Correlation between LLM-predicted and actual job satisfaction

</p>
:::

::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="ma_thesis/confusion_matrix.png" alt="turnover" class="zoom-image" style="cursor: pointer; display: block; margin: 0 auto; max-width: auto; height: 300px;"/>

<br>

<p style="font-weight: bold;">

*Figure 2*: Predicting Turnover Intention using LLM-embeddings

</p>
:::
:::

Additionally, I conducted exploratory analyses to provide practical insights into implementing LLMs. These analyses identified the optimal variations of BERT and hidden layers for accurately measuring job-related psychological attitudes. For a breakdown of some of the embedding process and feeding the embeddings to ml model see my blog titled For a detailed breakdown of the embedding process and how these embeddings are fed into machine learning models, check out my blog post titled ["From Text to Predictions"](https://sijan14.github.io/sijan_portfolio/posts/2024-05-21-LLM-BERT/).

<br>

::: {layout-ncol="2"}
```{r, echo=FALSE, warning=FALSE}
suppressMessages(library(plotly))
suppressMessages(library(dplyr))

# Data for BERTBASE performance comparison
data <- data.frame(
  TextEmbeddings = rep(c("Last Hidden", "Second-to-Last Hidden", "Concatenate Last Four Hidden", "Mean Last Four Hidden", "Sentence-level Mean Last Four Hidden"), each = 2),
  Metric = rep(c("Correlation", "RMSE"), times = 5),
  Value = c(0.62, 0.63, 0.62, 0.63, 0.63, 0.62, 0.64, 0.62, 0.69, 0.57),
  Level = rep(c("Word-level", "Word-level", "Word-level", "Word-level", "Sentence-level"), each = 2),
  Layer = rep(c("Layer 12", "Layer 11", "Concatenation of Layers 9-12", "Average of Layers 9-12", "Average of Layers 9-12"), each = 2)
)

# Calculate the mean correlation for each embedding type
order <- data %>%
  filter(Metric == "Correlation") %>%
  group_by(TextEmbeddings) %>%
  summarize(MeanCorrelation = mean(Value)) %>%
  arrange(MeanCorrelation) %>%
  pull(TextEmbeddings)

# Create a bar plot
fig <- plot_ly(
  data,
  x = ~factor(TextEmbeddings, levels = order), # Reorder categories by correlation
  y = ~Value,
  type = 'bar',
  color = ~Metric,
  colors = c("Correlation" = "#5C85FF", "RMSE" = "#F0B2F0"),
  text = ~paste("Metric:", Metric, "<br>Value:", Value, "<br>Level:", Level, "<br>Layer:", Layer),
  hoverinfo = 'text',
  barmode = 'group'
) %>%
  layout(
    title = "Performance Comparison of BERTBASE Hidden Layers",
    xaxis = list(
      title = "BERTBASE Layers (Ordered by Correlation)",
      showticklabels = FALSE # Hide reordered x-axis labels
    ),
    yaxis = list(title = "Value", range = c(0.5, 0.7)),
    legend = list(title = list(text = "Metric")),
    colorway = c("#636EFA", "#EF553B"),
    width = 560,
    height = 320
  )

fig
```

```{r, warning=FALSE, echo=FALSE}
# Data for BERT Variants performance comparison
data_variant <- data.frame(
  Model = c(
    "BERTBASE uncased", "BERTBASE cased", "BERTLARGE uncased", 
    "BERTLARGE cased", "DistilBERTBASE uncased", "DistilBERTBASE cased"
  ),
  Metric = rep(c("Correlation", "RMSE"), each = 6),
  Value = c(0.64, 0.67, 0.58, 0.66, 0.66, 0.62, 0.62, 0.60, 0.64, 0.60, 0.61, 0.62)
)

# Calculate the mean correlation for each model to order the bars
order_variant <- data_variant %>%
  filter(Metric == "Correlation") %>%
  arrange(Value) %>%
  pull(Model)

# Create a bar plot
fig_variant <- plot_ly(
  data_variant,
  x = ~factor(Model, levels = order_variant), # Reorder categories by correlation
  y = ~Value,
  type = 'bar',
  color = ~Metric,
  colors = c("Correlation" = "#FFA07A", "RMSE" = "#20B2AA"), # New color palette
  text = ~paste("Metric:", Metric, "<br>Value:", Value, "<br>Model:", Model),
  hoverinfo = 'text',
  barmode = 'group'
) %>%
  layout(
    title = "Performance Comparison of BERT Variants",
    xaxis = list(
      title = "BERT Variants (Ordered by Correlation)",
      showticklabels = FALSE # Show reordered x-axis labels
    ),
    yaxis = list(title = "Value", range = c(0.5, 0.7)),
    legend = list(title = list(text = "Metric")),
    colorway = c("#FFA07A", "#20B2AA"),
    width = 560,
    height = 320
  )

fig_variant

```

:::

### üï∞Ô∏è Delay Discounting as a Latent Factor

<p>[Date:]{style="font-weight: bold;"} December 15, 2024</p>

This project examines the latent factor structure of delay discounting, the tendency to prioritize immediate rewards over delayed ones, which is linked to behavioral outcomes such as substance abuse gambling, credit card debt, and poor academic performance (W√∂lfling et al., 2020). Previous studies have used various methods to measure delay discounting, but the findings have been inconsistent, partly due to differences in operationalization. This study uses confirmatory factor analysis (CFA) to explore the underlying latent factors of delay discounting and their relationship to behavioral outcomes, providing a clearer understanding of the construct and its implications.

<br>

::: {layout-ncol="3"}
::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="project_cfa/one_model.png" alt="One Factor Model" class="zoom-image" style="cursor: pointer; max-width: 100%; height: auto;"/>

<p style="font-weight: bold;">

One Factor Model:

</p>

<p>CFI = [.72]{style="color: red;"}</p>

<p>RMSEA = [.24]{style="color: red;"}</p>

<p>SRMR = [.109]{style="color: red;"}</p>

<p>Avg R<sup>2</sup> = [.60]{style="color: green;"}</p>
:::

::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="project_cfa/two_model.png" alt="One Factor Model" class="zoom-image" style="cursor: pointer; max-width: 100%; height: auto;"/>

<p style="font-weight: bold;">

Two Factor Model:

</p>

<p>CFI = [.94]{style="color: green;"}</p>

<p>RMSEA = [.12]{style="color: red;"}</p>

<p>SRMR = [.04]{style="color: green;"}</p>

<p>Avg R<sup>2</sup> = [.69]{style="color: green;"}</p>
:::

::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="project_cfa/four_model.png" alt="One Factor Model" class="zoom-image" style="cursor: pointer; max-width: 100%; height: auto;"/>

<p style="font-weight: bold;">

Four Factor Model:

</p>

<p>CFI = [.96]{style="color: green;"}</p>

<p>RMSEA = [.10]{style="color: green;"}</p>

<p>SRMR = [.04]{style="color: green;"}</p>

<p>Avg R<sup>2</sup> = [.69]{style="color: green;"}</p>
:::
:::

```{=html}
<style>
  /* CSS for zoom effect on hover */
  .zoom-image {
    transition: transform 0.3s ease-in-out, border 0.3s ease-in-out; /* Added border transition */
  }

  .zoom-image:hover {
    transform: scale(1.1);  /* Adjust the scale factor for zoom level */
    z-index: 10;
    border: 1px solid black;  /* Add black border on hover */
  }
</style>
```
<br>

------------------------------------------------------------------------

### ü§ñ Which ML Algorithms Predict Job Satisfaction The Best?

<p>[Date:]{style="font-weight: bold;"} May 2, 2023</p>

Machine learning algorithms have gained significant popularity in I/O psychology due to their advanced learning capabilities, often outperforming traditional regression methods in predictive tasks. However, their "black-box" nature remains a challenge for research justification. This project compares the performance of baseline model logistic regression with popular algorithms KNN, and random forest in a 4-class job satisfaction classification task using the IBM HR dataset from Kaggle, comprising approximately 23,000 observations. Using lasso-based feature-selection methods, hyperparameter tuning, the project optimizes model performance and identifies the algorithm with the highest predictive accuracy. The findings offer actionable insights into employee well-being, showcasing the potential of data-driven approaches to enhance workforce engagement and organizational performance.

::: {style="display: flex; justify-content: center;"}
```{r, echo=FALSE, warning=FALSE}
suppressMessages(library(plotly))

# Data for the algorithms
data <- data.frame(
  Algorithms = c("LOGIT", "KNN", "KNN (added features)", "KNN (repeated CV)", "Random Forest"),
  Precision = c(0.269, 0.682, 0.938, 0.938, 0.992),
  Sensitivity = c(0.231, 0.690, 0.941, 0.939, 0.994),
  Specificity = c(0.764, 0.897, 0.980, 0.979, 0.998),
  F1_Accuracy = c(0.206, 0.685, 0.939, 0.938, 0.993)
)

# Transform data for Plotly
data_long <- data.frame(
  Algorithms = rep(data$Algorithms, 4),
  Metrics = rep(c("Precision", "Sensitivity", "Specificity", "F1 Accuracy"), each = 5),
  Values = c(data$Precision, data$Sensitivity, data$Specificity, data$F1_Accuracy)
)

# Adjust the order of the metrics
data_long$Metrics <- factor(data_long$Metrics, levels = c("Precision", "Sensitivity", "Specificity", "F1 Accuracy"))

# Create a line plot with hover and dropdown functionality
fig <- plot_ly(
  data_long,
  x = ~Metrics,
  y = ~Values,
  type = 'scatter',
  mode = 'lines+markers',
  color = ~Algorithms,
  hoverinfo = 'text',
  text = ~paste("Algorithm:", Algorithms, "<br>Metric:", Metrics, "<br>Value:", Values)
) %>%
  layout(
    yaxis = list(title = "Value", range = c(0, 1)),
    xaxis = list(title = "Metrics"),
    width = 800,  # Set plot width
    height = 450, # Set plot height
    updatemenus = list(
      list(
        buttons = list(
          list(method = "restyle", args = list("visible", c(TRUE, FALSE, FALSE, FALSE, FALSE)), label = "KNN"),
          list(method = "restyle", args = list("visible", c(FALSE, TRUE, FALSE, FALSE, FALSE)), label = "KNN (added features)"),
          list(method = "restyle", args = list("visible", c(FALSE, FALSE, TRUE, FALSE, FALSE)), label = "KNN (repeated CV)"),
          list(method = "restyle", args = list("visible", c(FALSE, FALSE, FALSE, TRUE, FALSE)), label = "LOGIT"),
          list(method = "restyle", args = list("visible", c(FALSE, FALSE, FALSE, FALSE, TRUE)), label = "Random Forest"),
          list(method = "restyle", args = list("visible", c(TRUE, TRUE, TRUE, TRUE, TRUE)), label = "All Algorithms")
        ),
        direction = "down",
        showactive = TRUE,
        x = 0.8,
        y = 1.2
      )
    )
  )

fig
```
:::

<br>

------------------------------------------------------------------------

### ‚öñÔ∏è Conscientiousness Scale: Development & Validation

<p>[Date:]{style="font-weight: bold;"} April 21, 2023</p>

This project involved the psychometric development of a new Conscientiousness scale, one of the Big Five personality traits. Following best-practice item-writing guidelines, I conducted a pilot study and refined the item pool by removing items with low item-total correlations and minimal impact on Cronbach's alpha if removed (see Figure 1 & 2). Subsequent analyses demonstrated strong internal consistency (Œ± = .91) and validity evidence. The new scale exhibited high convergent validity (r = .85) with the well-validated IPIP Conscientiousness scale and good discriminant validity with other Big Five dimensions (see Figure 3). Criterion validity was supported by a positive correlation with job performance (r = .33), consistent with meta-analytic findings (Sackett et al., 2022), establishing the scale as a valid measure of conscientiousness.

<br>

::: {layout-ncol="3"}
::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="scale_dev/item_total_correlation.png" alt="One Factor Model" class="zoom-image" style="cursor: pointer; max-width: 100%; height: auto;"/>

<br>

<p style="font-weight: bold;">

Figure 1

</p>
:::

::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="scale_dev/item_dropped.png" alt="One Factor Model" class="zoom-image" style="cursor: pointer; max-width: 100%; height: auto;"/>

<br>

<p style="font-weight: bold;">

Figure 2

</p>
:::

::: {style="text-align:center; border: 1px black; padding: 10px; border-radius: 8px;"}
<img src="scale_dev/construct_validity.png" alt="One Factor Model" class="zoom-image" style="cursor: pointer; max-width: 100%; height: auto;"/>

<br>

<p style="font-weight: bold;">

Figure 3

</p>
:::
:::
