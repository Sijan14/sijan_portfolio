[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "What Are Data Structures? A Simple Introduction for Beginners\n\n\n\n\n\n\n\nData Science\n\n\n\n\nLearn what data structures are, why they matter, and how they can improve the efficiency of your code. This blog introduces key concepts in an easy-to-understand way for those just starting their coding journey.\n\n\n\n\n\n\nDec 20, 2024\n\n\nMd Allama Ikbal Sijan\n\n\n\n\n\n\n  \n\n\n\n\nR You Ready? An I/O Psychologist’s Guide to R & Rstudio\n\n\n\n\n\n\n\nR\n\n\nWorkshop\n\n\n\n\nMy January 16, 2024 METRO workshop is now a blog post, covering the essential 20% of R syntax for 80% of data analysis tasks. This blog post is designed for both R beginners and those seeking a refresher.\n\n\n\n\n\n\nJan 15, 2024\n\n\nMd Allama Ikbal Sijan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Md Allama Ikbal Sijan",
    "section": "",
    "text": "Hi there! I’m Sijan (Yess! I go by my last name-I will tell you the story someday!). I’m a Ph.D. candidate in Industrial-Organizational Psychology with a concentration in Data Science. I also teach undergraduate statistics and have a passion for discussing topics like assessment development, fairness, validity, psychometrics, data visualization, natural language processing (NLP), and artificial intelligence (AI). Poke around my website to learn more!\n\n \n\nwidget credit: richpauloo.github.io"
  },
  {
    "objectID": "posts/2024-12-20-data-structure-basics/index.html",
    "href": "posts/2024-12-20-data-structure-basics/index.html",
    "title": "What Are Data Structures? A Simple Introduction for Beginners",
    "section": "",
    "text": "When working with data, organizing, storing, and performing operations on it efficiently is key. This is where data structures come into play. In simple terms, a data structure is a way of organizing and managing data so that it can be accessed and modified efficiently.\n\nData Type vs. Data Structure\nIf you are reading this, you’re likely already familiar with data types such as int (integer), float (floating-point number), str (string), and bool (Boolean). These are the building blocks used to define the kind of data you’re working with. When you perform data cleaning or analysis, you often manipulate these primitive data types.\nData structures build on these basic data types to manage and organize data more effectively. Think of it like constructing a car: the raw materials like metal and plastic are akin to the primitive data types, while the car’s components, such as the engine, trunk, doors, and wheels, represent the data structures. In essence, data structures use data types to create organized frameworks that allow for efficient storage and operations on your data.\n\n\nCommon Operations\nData structures are essential tools in programming because they allow you to store, organize, and manage data efficiently. Common operations performed with data structures include:\n\nInsertion: Adding new elements.\nDeletion: Removing existing elements.\nSearching: Finding specific elements.\nAccessing: Retrieving elements by their position or key.\nUpdating: Modifying existing elements.\n\nWhile data types tell the computer what kind of data is being stored (e.g., integers, strings, floats), they do not inherently provide a way to store multiple values together or perform advanced operations. For managing multiple similar data points, data structures become invaluable.\n\n\nWhy Use Data Structures?\nIf you have several similar pieces of data to store and manipulate later, using a data structure allows for organized and efficient operations. Here’s an example:\n\n\n\nExample: Storing Student Grades\nImagine you have 5 students in a class who took a final exam. Their grades need to be stored for future calculations.\n\nWithout a Data Structure\nUsing only data types, you could store the grades in separate variables like this:\nstudent1 = 85\nstudent2 = 90\nstudent3 = 78\nstudent4 = 92\nstudent5 = 88\nThis approach works but becomes cumbersome as the number of students increases. Operations like calculating the average grade or updating a grade require manual effort for each variable.\n\n\nWith a Data Structure (Array)\nInstead, you can use an array to store the grades:\nstudent_grades = [85, 90, 78, 92, 88]\nNow you can perform operations more efficiently:\n# Access a specific grade: Retrieve the second student's grade\nsecond_grade = student_grades[1]  # Outputs 90\n\n# Update a grade: Add extra credit to the third student's grade\nstudent_grades[2] += 5  # Updates 78 to 83\n\n# Calculate the average grade: Compute the average for all students\naverage_grade = sum(student_grades) / len(student_grades)  # Outputs 86.6\n\n# Add a new student: Include another student's grade\nstudent_grades.append(95)\n\n# Remove a student: Remove the grade of a student who dropped out\nstudent_grades.pop(1)  # Removes the second student's grade\n\n\nWith a Data Structure (Hash Table)\nIf you need to associate grades with student names for better clarity, a hash table (dictionary) works even better:\nstudent_grades = {\n    \"Sijan\": 85,\n    \"Bella\": 90,\n    \"Alex\": 78,\n    \"Taylor\": 92,\n    \"Jordan\": 88\n}\nWith this structure:\n\nAccessing a grade by name is straightforward:\n\nbella_grade = student_grades[\"Bella\"]  # Outputs 90\n\nUpdating grades is intuitive:\n\nstudent_grades[\"Alex\"] += 5  # Updates Alex's grade to 83\n\nAdding or removing students is easy:\n\nstudent_grades[\"Chris\"] = 95  # Adds Chris with a grade of 95\ndel student_grades[\"Taylor\"]  # Removes Taylor\n\n\n\nCommon Data Structures\nUnderstanding the unique characteristics of different data structures—whether they are ordered or unordered, and what operations they excel at—helps in selecting the best one for a given task.\n\nArray: An ordered collection of elements, best for quick access and iteration.\nLinked List: An ordered, dynamic structure where elements are linked, ideal for frequent insertions and deletions.\nTrees: A hierarchical, ordered structure, excellent for representing hierarchical relationships and performing searches.\nHash Table: An unordered structure using key-value pairs, best for fast lookups and retrievals.\nHeap: A specialized tree-based structure, great for efficient retrieval of the smallest or largest element.\nGraphs: A network of nodes connected by edges, perfect for modeling relationships and performing pathfinding operations.\n\n\n\nData Structures vs. Abstract Data Types (ADT)\nEven if you’re not very familiar with the different data structures I’ve mentioned, you’re likely familiar with Abstract Data Types (ADTs) like lists, sets, and dictionaries. ADTs essentially define the operations or functionalities you can perform on data, while data structures are the low-level implementations that make those operations possible. Using the car example again, the ADT represents the car’s functionalities—driving, braking, and steering—while the data structures are the specific building blocks like the engine, wheels, and frame that make those functionalities work. ADTs are high-level abstract concepts, while data structures are the concrete details that enable those concepts to function.\nKnowing just the ADT and its functionality will take you a long way, and you don’t necessarily need to understand the underlying data structures to perform different tasks. For instance, a list can be implemented using either an array or a linked list. When you use the built-in list function, like my_list = [], you don’t need to know whether it’s implemented using an array or a linked list. However, understanding the underlying data structure can be important for optimizing performance, such as improving runtime or space complexity. This deeper understanding helps you make more efficient decisions when choosing how to implement solutions, especially when performance is critical.\n\n\nCommon ADTs\nHere are some common ADTs and the data structures used to implement them:\n\nList: Array, Linked List\nStack: Array, Linked List\nQueue: Array, Linked List, Circular Buffer\nDeque: Array, Doubly Linked List\nSet: Hash Table, BST\nBag: Array, Linked List\nPriority Queue: Binary Heap, Fibonacci Heap\nDictionary: Hash Table, BST\n\n\n\nReal World Applications\nDifferent data structures are utilized in real-world applications to optimize performance, manage data efficiently, and solve complex problems. Here are some practical implementations of commonly used data structures:\n\nArray: Arrays are used in applications like image processing and spreadsheets where data is stored in contiguous memory locations for quick access.\nLinked List: Linked lists are utilized in web browsers to manage navigation history, allowing for easy forward and backward movement.\nStack: Stacks are implemented in function call management, as well as in undo/redo operations in text editors, following the Last In, First Out (LIFO) principle.\nQueue: Queues are used in operating systems for task scheduling and in customer service systems to process requests in the order they arrive.\nSet: Sets are applied in social media platforms to store unique user data, such as friend lists, without duplicates.\nHash Table: Hash tables are used in contact management systems to map a user’s name to their phone number for fast lookups.\nBinary Search Tree (BST): BSTs are used in databases for fast searching, insertion, and deletion of sorted records.\nHeap: Heaps are implemented in job scheduling systems to prioritize tasks based on urgency or importance.\nGraph: Graphs are used to model networks like social media connections, transportation systems, and web pages.\n\n\n\nConclusion\nThese are just a few examples of how data structures are applied in the real world, and there are countless other uses that I haven’t covered here. It took me some time to truly grasp the key differences between data structures and other data types. This brief introduction to data structures serves as a way for me to reinforce my own understanding and, hopefully, help others in the process.\nData structures are fundamental tools for organizing and managing data efficiently across various real-world applications. From simple structures like arrays and linked lists to more complex ones like hash tables and graphs, each data structure has its own purpose based on the specific problem you’re trying to solve. Understanding these practical applications enables us to select the right data structure, optimize performance, and tackle challenges more effectively.\nIf you found this post helpful or have any feedback, I’d love to hear your thoughts—feel free to leave a comment below!\n\n\nReference\nzyBooks, a Wiley brand. (2024). Data Structures With Python. https://learn.zybooks.com/zybook/MONTCLAIRCSIT506ZharriFall2024 (accessed 2024).\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{allama ikbal sijan2024,\n  author = {Allama Ikbal Sijan, Md},\n  title = {What {Are} {Data} {Structures?} {A} {Simple} {Introduction}\n    for {Beginners}},\n  date = {2024-12-20},\n  url = {https://sijan14.github.io/sijan_portfolio/posts/2024-12-20-data-structure-basics/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAllama Ikbal Sijan, Md. 2024. “What Are Data Structures? A Simple\nIntroduction for Beginners.” December 20, 2024. https://sijan14.github.io/sijan_portfolio/posts/2024-12-20-data-structure-basics/."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Delay Discounting as a Latent Factor\n\nDate: December 15, 2024\n\nThis project examines the latent factor structure of delay discounting, the tendency to prioritize immediate rewards over delayed ones, which is linked to behavioral outcomes such as substance abuse gambling, credit card debt, and poor academic performance (Wölfling et al., 2020). Previous studies have used various methods to measure delay discounting, but the findings have been inconsistent, partly due to differences in operationalization. This study uses confirmatory factor analysis (CFA) to explore the underlying latent factors of delay discounting and their relationship to behavioral outcomes, providing a clearer understanding of the construct and its implications.\n\n\n\n\n\n\nOne Factor Model:\n\n\nCFI = .72\n\n\nRMSEA = .24\n\n\nSRMR = .109\n\n\nAvg R2 = .60\n\n\n\n\n\nTwo Factor Model:\n\n\nCFI = .94\n\n\nRMSEA = .12\n\n\nSRMR = .04\n\n\nAvg R2 = .69\n\n\n\n\n\nFour Factor Model:\n\n\nCFI = .96\n\n\nRMSEA = .10\n\n\nSRMR = .04\n\n\nAvg R2 = .69\n\n\n\n\n\n\n\n\n\nWhich ML Algorithms Predict Job Satisfaction The Best?\n\nDate: May 2, 2023\n\nMachine learning algorithms have gained significant popularity in I/O psychology due to their advanced learning capabilities, often outperforming traditional regression methods in predictive tasks. However, their “black-box” nature remains a challenge for research justification. This project compares the performance of baseline model logistic regression with popular algorithms KNN, and random forest in a 4-class job satisfaction classification task using the IBM HR dataset from Kaggle, comprising approximately 23,000 observations. Using lasso-based feature-selection methods, hyperparameter tuning, the project optimizes model performance and identifies the algorithm with the highest predictive accuracy. The findings offer actionable insights into employee well-being, showcasing the potential of data-driven approaches to enhance workforce engagement and organizational performance.\n\n\n\n\n\n\n\n\n\n\n\n\nA Psychometric Scale for Conscientiousness: Development and Validation\n\nDate: April 21, 2023\n\nThis project involved the psychometric development of a new Conscientiousness scale, one of the Big Five personality traits. Following best-practice item-writing guidelines, I conducted a pilot study and refined the item pool by removing items with low item-total correlations and minimal impact on Cronbach’s alpha if removed (see Figure 1 & 2). Subsequent analyses demonstrated strong internal consistency (α = .91) and validity evidence. The new scale exhibited high convergent validity (r = .85) with the well-validated IPIP Conscientiousness scale and good discriminant validity with other Big Five dimensions (see Figure 3). Criterion validity was supported by a positive correlation with job performance (r = .33), consistent with meta-analytic findings (Sackett et al., 2022), establishing the scale as a valid measure of conscientiousness.\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there! I’m Sijan, a Ph.D. candidate in Industrial-Organizational Psychology with a concentration in Data Science at Montclair State University. In other words, I’m passionate about people, data, and making workplaces better for everyone.\n\n\nI blend psychology with cutting-edge data science techniques to create psychometrically valid, reliable assessment tools. By leveraging natural language processing (NLP) and large language models (LLMs), I design psychometrically robust tools to predict key organizational metrics like job satisfaction, engagement, and turnover. My most recent work focuses on examining the validity, predictive bias, and risk of adverse impact of AI-based assessment tools and scoring systems. I also explore topics like team decision-making, delay discounting, and biases and heuristics.\n\n\n\nWhen I’m not diving into data or teaching statistics as an adjunct professor, I’m leading initiatives and workshops for METRO, SIO NYC, and Eagle I/O, empowering I/O students and professionals. I’ve also worked with organizations like JUST Capital and City Bank, helping them make data-driven decisions to enhance their workforce strategies."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hey there! I’m Sijan, a Ph.D. candidate in Industrial-Organizational Psychology with a concentration in Data Science at Montclair State University. In other words, I’m passionate about people, data, and making workplaces better for everyone.\n\n\nI blend psychology with cutting-edge data science techniques to create psychometrically valid, reliable assessment tools. By leveraging natural language processing (NLP) and large language models (LLMs), I design psychometrically robust tools to predict key organizational metrics like job satisfaction, engagement, and turnover. My most recent work focuses on examining the validity, predictive bias, and risk of adverse impact of AI-based assessment tools and scoring systems. I also explore topics like team decision-making, delay discounting, and biases and heuristics.\n\n\n\nWhen I’m not diving into data or teaching statistics as an adjunct professor, I’m leading initiatives and workshops for METRO, SIO NYC, and Eagle I/O, empowering I/O students and professionals. I’ve also worked with organizations like JUST Capital and City Bank, helping them make data-driven decisions to enhance their workforce strategies."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "🎓 Education",
    "text": "🎓 Education\n\nDoctor of Philosophy, Industrial-Organizational Psychology (Concentration: Data Science)\nMontclair State University | 2022 - 2026\nGraduate Certificate in Advanced Quantitative Methods\nMontclair State University | 2022 - 2025\nMaster of Arts, Industrial-Organizational Psychology\nMontclair State University | 2022 - 2024\nBachelor of Arts, Global Management & Psychology\nEarlham College | 2018 - 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "🧑🏽‍💻 Experience",
    "text": "🧑🏽‍💻 Experience\n\nDoctoral Research Assistant, Montclair State University | August 2022 - Present\nAdjunct Professor, Montclair State University | September 2024 - Present\nCo-Director of Education, METRO | October 2023 - Present\nFounding Board Member, SIONYC | September 2023 - May 2024\nPresident & Consultant, Eagle I.O | September 2023 - May 2024\nResearch Intern, JUST Capital | May 2023 - August 2023\nGraduate Assistant, Center for Writing Excellence | September 2022 - May 2023\nConsulting & Data Analytics Extern, Paragon One | June 2022 - September 2022\nHR Reporting Intern, City Bank Limited | May 2021 - August 2021"
  },
  {
    "objectID": "about.html#presentations",
    "href": "about.html#presentations",
    "title": "About",
    "section": "🎙️Presentations",
    "text": "🎙️Presentations\n\nSijan, M. (Co-chair), Belwalkar, B. B. (Co-chair) & Mracek, D. (Discussant) (2025). Power of NLP and LLMs: Turning I/O Research and Practice into a Textual Adventure [Symposium]. Society for Industrial and Organizational Psychology Annual Conference, Denver, CO, United States.*\nHuynh, C., Elfeki, Y., Sijan, M. (2025). Modeling Latent Constructs in SJT Using Pseudo-Factor Analysis [Poster]. Society for Industrial and Organizational Psychology Annual Conference, Denver, CO, United States.*\nSijan, M., Bixter, M. T. (2024). Using NLP to Predict Job Satisfaction and Turnover Intention [Poster Presentation]. The Psychonomic Society Annual Meeting, New York, NY, United States.\nBragger, J., Helisch, R., Buczek, E., Hunt, L., Sijan, M., Abbasi, F., Filstein, T., Myszko, Z., Sherman, C., & Lakusta, L. (2024). Neurodiversity in Leadership: The Relationship Between Theory of Mind and Leadership Roles, and Possible Implications for Neurodivergent Workers [Presentation]. Interdisciplinary Perspectives on Leadership Symposium, Thessaloniki, Greece.\nSijan, M., Kramer, M. (Co-Chair), Kulas, J. (Co-Chair), Helisch, R., Lancuna, L., Defabiis, M., Hunt, L., Notari, K., (2024). Unlocking the value of IO Psychology Student Consulting in Higher Education [Panel Presentation]. Society for Industrial and Organizational Psychology Annual Conference, Chicago, IL, United States.\nElfeki, Y., Huynh, C., Sijan, M., Salter, N. P. (2024). An Exploration of Masculinity and Femininity Perceptions Across Monoracial and Biracial Identities using StyleGAN Imagery [Symposium]. Society for Industrial and Organizational Psychology Annual Conference, Chicago, IL, United States.\nHunt, L., Bragger, J., Kubu, G., Abbasi, F., & Sijan, M. (2024). Leading the Goals’ Rush: How leadership mindsets make all the difference [Poster Presentation]. Society for Industrial and Organizational Psychology Annual Conference, Chicago, IL, United States.\nSijan M. (2022). Perception of Trust Based on Parenthood Status [Poster Presentation]. 94th Annual MPA (Midwestern Psychological Association) Conference. Chicago, IL, United States."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html",
    "href": "posts/2024-12-31-R-Workshop/index.html",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "",
    "text": "In recent years, the R programming language emerged as the industry standard in social science research, owing to its robust statistical analysis tools, powerful data visualization capabilities, and a rich ecosystem of packages. Its open-source nature and emphasis on reproducibility further propelled its widespread adoption. This workshop, titled “R You Ready? An I/O Psychologist’s Guide to R and RStudio,” was tailored for both R novices and those seeking a refresher, focusing on the essential 20% of R syntax that facilitated 80% of data analysis tasks. Participants were introduced to the RStudio environment and fundamental R syntax, followed by hands-on sessions covering data transformation (dplyr), data visualization (ggplot2), statistical analyses (lm, stats), APA-style reporting (markdown, apa, papaja), and text mining (stringr, tidytext). The workshop concluded with collaborative problem-solving sessions to reinforce the concepts learned. The aim was to introduce students and practitioners to the power of R to carry out basic and advanced analyses, along with the capability to visualize and report findings."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-1",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-1",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #1",
    "text": "Tutorial #1\n\nGetting Started\nBefore diving into the transformations, let’s load the necessary package and dataset:\n\n# Uncomment the next line if you haven't installed the package\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# loading the dataset\ndf &lt;- read_csv(\"IBM HR Data.csv\")\n\n# checking all the column names\nnames(df)\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"Application ID\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n[37] \"Employee Source\"         \n\n\n\n\n\nSorting Data\nSorting is often one of the first transformations you’ll perform. Here’s how to do it in base R and tidyverse.\n\nOption 1: Base R\nIn base R, you can use the order() function to sort by one or more columns:\n\n# sort by age in ascending order\nindex &lt;- order(df$Age) \ndf &lt;- df[index, ]\n\n\n\nOption 2: Tidyverse\nThe arrange() function from dplyr is a more concise and readable option:\n\n# Sort by multiple columns: Age (descending) and DailyRate (ascending)\ndf &lt;- arrange(df, desc(Age), DailyRate)\n\n\n\n\n\nRenaming Columns\nRenaming columns is another frequent task, especially when preparing data for analysis or visualization.\n\nOption 1: Base R\nUse the colnames() function to rename specific columns:\n\n# Rename the first column (age)\ncolnames(df)[1] &lt;- \"how_old\"\n\n\n\nOption 2: Tidyverse\nThe rename() function provides an intuitive way to rename columns:\n\n# Rename \"how_old\" back to \"age\"\ndf &lt;- rename(df, \"age\" = \"how_old\")\n\n\n\n\n\nFiltering Data\nFiltering rows based on specific conditions is crucial for focusing your analysis. Let’s filter for single employees earning over $10,000 per month:\n\n# Filter rows where MaritalStatus is \"Single\" and MonthlyIncome &gt; 10000\nsingle_rich &lt;- df %&gt;%\n  filter(MaritalStatus == \"Single\", MonthlyIncome &gt; 10000)\n\nThe pipe operator (%&gt;%) from the tidyverse makes it easy to chain multiple operations.\n\n\n\nSummarizing Data\nSummarization provides insights into key metrics like mean and standard deviation. Here’s how to summarize the filtered data:\n\n# summarizing the subset data\nsingle_rich %&gt;%\n  summarise(Mean = mean(MonthlyIncome), SD = sd(MonthlyIncome))\n\n# A tibble: 1 × 2\n    Mean    SD\n   &lt;dbl&gt; &lt;dbl&gt;\n1 15043. 3200.\n\n\nThis calculates the average (Mean) and standard deviation (SD) of the MonthlyIncome column for the filtered subset.\n\n\n\nSubsetting Columns\nSelecting specific columns is often necessary to focus on relevant data or remove unnecessary variables. You can use the select() function from the tidyverse to keep only the desired columns:\n\n# Keep specific columns\ndf %&gt;%\n  select(Attrition, DailyRate, Department, Education)\n\n# A tibble: 23,532 × 4\n   Attrition        DailyRate Department             Education\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 Current employee       370 Research & Development         4\n 2 Current employee       370 Research & Development         4\n 3 Current employee       370 Research & Development         4\n 4 Current employee       370 Research & Development         4\n 5 Current employee       370 Research & Development         4\n 6 Current employee       370 Research & Development         4\n 7 Current employee       370 Research & Development         4\n 8 Current employee       370 Research & Development         4\n 9 Current employee       370 Research & Development         4\n10 Current employee       370 Research & Development         4\n# ℹ 23,522 more rows\n\n\nThis selects only the Attrition, DailyRate, Department, and Education columns.\nTo drop specific columns, use the select() function with the negation operator !:\n\n# Drop unnecessary columns\ndf &lt;- df %&gt;%\n  select(!c(EmployeeCount, StandardHours, Over18))\n\nThis removes the EmployeeCount, StandardHours, and Over18 columns.\n\n\nGrouping Data by Variable\nGrouping rows is helpful for summarizing data by categories. Here, we’ll group the data by EducationField and count the number of occurrences in each category.\n\n# Group by EducationField and count rows\ndf %&gt;%\n  group_by(EducationField) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 × 2\n# Groups:   EducationField [9]\n  EducationField       n\n  &lt;chr&gt;            &lt;int&gt;\n1 Life Sciences     9725\n2 Medical           7393\n3 Marketing         2541\n4 Technical Degree  2105\n5 Other             1311\n6 Human Resources    446\n7 &lt;NA&gt;                 9\n8 3                    1\n9 Test                 1\n\n\nThis code:\n\nGroups the data by EducationField.\nCounts the rows in each group.\nArranges the groups in descending order by count (n).\n\n\n\nRecoding Variables\nRecoding variables is a common task when transforming categorical data into numerical or more meaningful labels. First, let’s check the frequency of each category in the Attrition column:\n\n# View the distribution of Attrition\ntable(df$Attrition)\n\n\n     Current employee           Termination Voluntary Resignation \n                19714                    96                  3709 \n\n\nYou can use the recode() function from the tidyverse to assign new values to the categories:\n\n# Recode Attrition variable\ndf$Attrition &lt;- recode(df$Attrition,\n                       \"Current employee\" = 0,\n                       \"Termination\" = 1,\n                       \"Voluntary Resignation\" = 2)\n\nThis transforms Attrition into a numerical variable with the following mapping:\n\n0: Current employee\n1: Termination\n2: Voluntary Resignation"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #2",
    "text": "Tutorial #2\nIn this tutorial, we will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere’s how to create a simple scatterplot of YearsAtCompany vs. TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we’ll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere’s what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#scatterplots-in-ggplot2",
    "href": "posts/2024-12-31-R-Workshop/index.html#scatterplots-in-ggplot2",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Scatterplots in ggplot2",
    "text": "Scatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\n\nBasic Scatterplot\nHere’s how to create a simple scatterplot of YearsAtCompany vs. TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#introduction",
    "href": "posts/2024-12-31-R-Workshop/index.html#introduction",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "",
    "text": "In recent years, the R programming language emerged as the industry standard in social science research, owing to its robust statistical analysis tools, powerful data visualization capabilities, and a rich ecosystem of packages. Its open-source nature and emphasis on reproducibility further propelled its widespread adoption. This workshop, titled “R You Ready? An I/O Psychologist’s Guide to R and RStudio,” was tailored for both R novices and those seeking a refresher, focusing on the essential 20% of R syntax that facilitated 80% of data analysis tasks. Participants were introduced to the RStudio environment and fundamental R syntax, followed by hands-on sessions covering data transformation (dplyr), data visualization (ggplot2), statistical analyses (lm, stats), APA-style reporting (markdown, apa, papaja), and text mining (stringr, tidytext). The workshop concluded with collaborative problem-solving sessions to reinforce the concepts learned. The aim was to introduce students and practitioners to the power of R to carry out basic and advanced analyses, along with the capability to visualize and report findings."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-data-cleaning",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-data-cleaning",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #1 (Data Cleaning)",
    "text": "Tutorial #1 (Data Cleaning)\n\nGetting Started\nBefore diving into the transformations, let’s load the necessary package and dataset:\n\n# Uncomment the next two lines if you haven't installed the package\n# install.packages(\"tidyverse\")\n# install.packages(\"psych\")\n# install.packages(\"lsr\")\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(lsr)\n\n# Loading the dataset\ndf &lt;- read_csv(\"IBM HR Data.csv\")\n\n# Checking all the column names\nnames(df)\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"Application ID\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n[37] \"Employee Source\"         \n\n\n\n\n\nSorting Data\nSorting is often one of the first transformations you’ll perform. Here’s how to do it in base R and tidyverse.\n\nOption 1: Base R\nIn base R, you can use the order() function to sort by one or more columns:\n\n# Sort by age in ascending order\nindex &lt;- order(df$Age) \ndf &lt;- df[index, ]\n\n\n\nOption 2: Tidyverse\nThe arrange() function from dplyr is a more concise and readable option:\n\n# Sort by multiple columns: Age (descending) and DailyRate (ascending)\ndf &lt;- arrange(df, desc(Age), DailyRate)\n\n\n\n\n\nRenaming Columns\nRenaming columns is another frequent task, especially when preparing data for analysis or visualization.\n\nOption 1: Base R\nUse the colnames() function to rename specific columns:\n\n# Rename the first column (age)\ncolnames(df)[1] &lt;- \"how_old\"\n\n\n\nOption 2: Tidyverse\nThe rename() function provides an intuitive way to rename columns:\n\n# Rename \"how_old\" back to \"age\"\ndf &lt;- rename(df, \"age\" = \"how_old\")\n\n\n\n\n\nFiltering Data\nFiltering rows based on specific conditions is crucial for focusing your analysis. Let’s filter for single employees earning over $10,000 per month:\n\n# Filter rows where MaritalStatus is \"Single\" and MonthlyIncome &gt; 10000\nsingle_rich &lt;- df %&gt;%\n  filter(MaritalStatus == \"Single\", MonthlyIncome &gt; 10000)\n\nThe pipe operator (%&gt;%) from the tidyverse makes it easy to chain multiple operations.\n\n\n\nSummarizing Data\nSummarization provides insights into key metrics like mean and standard deviation. Here’s how to summarize the filtered data:\n\n# summarizing the subset data\nsingle_rich %&gt;%\n  summarise(Mean = mean(MonthlyIncome), SD = sd(MonthlyIncome))\n\n# A tibble: 1 × 2\n    Mean    SD\n   &lt;dbl&gt; &lt;dbl&gt;\n1 15043. 3200.\n\n\nThis calculates the average (Mean) and standard deviation (SD) of the MonthlyIncome column for the filtered subset.\n\n\n\nSubsetting Columns\nSelecting specific columns is often necessary to focus on relevant data or remove unnecessary variables. You can use the select() function from the tidyverse to keep only the desired columns:\n\n# Keep specific columns\ndf %&gt;%\n  select(Attrition, DailyRate, Department, Education)\n\n# A tibble: 23,532 × 4\n   Attrition        DailyRate Department             Education\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 Current employee       370 Research & Development         4\n 2 Current employee       370 Research & Development         4\n 3 Current employee       370 Research & Development         4\n 4 Current employee       370 Research & Development         4\n 5 Current employee       370 Research & Development         4\n 6 Current employee       370 Research & Development         4\n 7 Current employee       370 Research & Development         4\n 8 Current employee       370 Research & Development         4\n 9 Current employee       370 Research & Development         4\n10 Current employee       370 Research & Development         4\n# ℹ 23,522 more rows\n\n\nThis selects only the Attrition, DailyRate, Department, and Education columns.\nTo drop specific columns, use the select() function with the negation operator !:\n\n# Drop unnecessary columns\ndf &lt;- df %&gt;%\n  select(!c(EmployeeCount, StandardHours, Over18))\n\nThis removes the EmployeeCount, StandardHours, and Over18 columns.\n\n\nGrouping Data by Variable\nGrouping rows is helpful for summarizing data by categories. Here, we’ll group the data by EducationField and count the number of occurrences in each category.\n\n# Group by EducationField and count rows\ndf %&gt;%\n  group_by(EducationField) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 × 2\n# Groups:   EducationField [9]\n  EducationField       n\n  &lt;chr&gt;            &lt;int&gt;\n1 Life Sciences     9725\n2 Medical           7393\n3 Marketing         2541\n4 Technical Degree  2105\n5 Other             1311\n6 Human Resources    446\n7 &lt;NA&gt;                 9\n8 3                    1\n9 Test                 1\n\n\nThis code:\n\nGroups the data by EducationField.\nCounts the rows in each group.\nArranges the groups in descending order by count (n).\n\n\n\nRecoding Variables\nRecoding variables is a common task when transforming categorical data into numerical or more meaningful labels. First, let’s check the frequency of each category in the Attrition column:\n\n# View the distribution of Attrition\ntable(df$Attrition)\n\n\n     Current employee           Termination Voluntary Resignation \n                19714                    96                  3709 \n\n\nYou can use the recode() function from the tidyverse to assign new values to the categories:\n\n# Recode Attrition variable\ndf$Attrition &lt;- recode(df$Attrition,\n                       \"Current employee\" = 0,\n                       \"Termination\" = 1,\n                       \"Voluntary Resignation\" = 2)\n\nThis transforms Attrition into a numerical variable with the following mapping:\n\n0: Current employee\n1: Termination\n2: Voluntary Resignation"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-data-visualization",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-data-visualization",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #2 (Data Visualization)",
    "text": "Tutorial #2 (Data Visualization)\nIn this tutorial, we will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere’s how to create a simple scatterplot of YearsAtCompany vs. TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we’ll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere’s what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-3-analysis",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-3-analysis",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #3 (Analysis)",
    "text": "Tutorial #3 (Analysis)\n\nSummary and Frequency Tables\nUse summary() and table() to calculate basic descriptive statistics and frequency counts.\n\n# Descriptive statistics for numeric variables\nsummary(df$WorkLifeBalance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   3.000   2.761   3.000   4.000      10 \n\n# Frequency table for categorical variables\ntable(df$Gender)\n\n\n     1      2 Female   Male \n     1      1   9400  14120 \n\n\nUsing the describe function from the psych Package for Detailed Summaries of numeric variables\n\n# Descriptive statistics for MonthlyIncome\ndescribe(df$MonthlyIncome)\n\n   vars     n    mean      sd median trimmed     mad  min   max range skew\nX1    1 23516 6502.76 4705.99   4936 5666.19 3278.03 1009 19999 18990 1.37\n   kurtosis    se\nX1        1 30.69\n\n\n\n\n\nt-tests\nA t-test is used to compare the means of two groups. Here, we examine differences in JobSatisfaction between genders.\n\n# Filtering Gender\ndf &lt;- df %&gt;%\n  filter(Gender %in% c(\"Female\", \"Male\"))\n\n# Perform t-test\nt.test(df$JobSatisfaction ~ df$Gender, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  df$JobSatisfaction by df$Gender\nt = -5.0624, df = 23511, p-value = 4.171e-07\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.10296313 -0.04548644\nsample estimates:\nmean in group Female   mean in group Male \n            2.681247             2.755472 \n\n\n\n\nEffect Size\nEffect size provides a measure of the magnitude of differences. We calculate Cohen’s D using the lsr package.\n\n# Cohen's D for JobSatisfaction by Gender\ncohensD(df$JobSatisfaction ~ df$Gender)\n\n[1] 0.0674014\n\n\n\n\n\n\nOne-Way ANOVA\nANOVA (Analysis of Variance) is used to compare the means of more than two groups. We analyze JobSatisfaction across Department.\n\n# Convert Department to factor\nclass(df$Department) # Check class\n\n[1] \"character\"\n\ndf$Department &lt;- as.factor(df$Department) # Convert to factor\n\n# Conduct ANOVA\nanovaTest &lt;- aov(df$JobSatisfaction ~ df$Department)\nsummary(anovaTest)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)\ndf$Department     2      1   0.363   0.299  0.742\nResiduals     23499  28524   1.214               \n18 observations deleted due to missingness\n\n\n\nEffect Size\nafter conducting the ANOVA, we apply Tukey’s HSD to compare the JobSatisfaction between different levels of Department.\n\n# Post-hoc Tukey's HSD test\nTukeyHSD(anovaTest)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = df$JobSatisfaction ~ df$Department)\n\n$`df$Department`\n                                              diff         lwr        upr\nResearch & Development-Human Resources 0.021787253 -0.06179058 0.10536508\nSales-Human Resources                  0.027838399 -0.05866972 0.11434652\nSales-Research & Development           0.006051146 -0.03093437 0.04303666\n                                           p adj\nResearch & Development-Human Resources 0.8141109\nSales-Human Resources                  0.7310521\nSales-Research & Development           0.9221490\n\n\n\n\n\n\nCorrelation Analysis\nThe correlation between two continuous variables can be calculated using the cor() or cor.test function. Here, we calculate the correlation between JobInvolvement and RelationshipSatisfaction.\n\n# Calculate correlation, using complete observations only\ncor.test(df$JobInvolvement, df$RelationshipSatisfaction, use = \"complete.obs\")\n\n\n    Pearson's product-moment correlation\n\ndata:  df$JobInvolvement and df$RelationshipSatisfaction\nt = 5.1325, df = 23506, p-value = 2.882e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.02068330 0.04622137\nsample estimates:\n      cor \n0.0334578 \n\n\n\n\n\nMultiple Linear Regression\nA multiple linear regression model can be used to predict the value of a continuous dependent variable based on multiple independent variables. Here, we build a model predicting JobSatisfaction.\n\n# Multiple linear regression model\nmodel1 &lt;- lm(JobSatisfaction ~ \n              age + Gender + JobLevel + MonthlyIncome, \n            data = df)\n\n# Display the summary of the model\nsummary(model1)\n\n\nCall:\nlm(formula = JobSatisfaction ~ age + Gender + JobLevel + MonthlyIncome, \n    data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0440 -0.7585  0.2526  1.2248  1.4585 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.777e+00  3.336e-02  83.237  &lt; 2e-16 ***\nage           -3.597e-03  8.127e-04  -4.426 9.63e-06 ***\nGenderMale     7.321e-02  1.468e-02   4.989 6.13e-07 ***\nJobLevel       6.273e-02  1.948e-02   3.220  0.00128 ** \nMonthlyIncome -1.412e-05  4.568e-06  -3.090  0.00200 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.101 on 23490 degrees of freedom\n  (25 observations deleted due to missingness)\nMultiple R-squared:  0.002353,  Adjusted R-squared:  0.002183 \nF-statistic: 13.85 on 4 and 23490 DF,  p-value: 2.77e-11\n\n# Second model including EducationField as a factor variable\nmodel2 &lt;- lm(JobSatisfaction ~ \n               as.factor(EducationField) + \n               age + Gender + JobLevel + \n               MonthlyIncome, \n             data = df)\n\n# Display the summary of the model\nsummary(model2)\n\n\nCall:\nlm(formula = JobSatisfaction ~ as.factor(EducationField) + age + \n    Gender + JobLevel + MonthlyIncome, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0400 -0.7611  0.2494  1.2052  1.5089 \n\nCoefficients:\n                                            Estimate Std. Error t value\n(Intercept)                                2.683e+00  6.190e-02  43.347\nas.factor(EducationField)Life Sciences     1.459e-01  5.333e-02   2.735\nas.factor(EducationField)Marketing         8.328e-02  5.653e-02   1.473\nas.factor(EducationField)Medical           5.383e-02  5.370e-02   1.002\nas.factor(EducationField)Other             8.145e-02  6.036e-02   1.350\nas.factor(EducationField)Technical Degree  5.170e-02  5.740e-02   0.901\nas.factor(EducationField)Test              3.908e-01  1.101e+00   0.355\nage                                       -3.670e-03  8.132e-04  -4.513\nGenderMale                                 7.362e-02  1.467e-02   5.018\nJobLevel                                   6.450e-02  1.948e-02   3.311\nMonthlyIncome                             -1.453e-05  4.568e-06  -3.180\n                                          Pr(&gt;|t|)    \n(Intercept)                                &lt; 2e-16 ***\nas.factor(EducationField)Life Sciences    0.006240 ** \nas.factor(EducationField)Marketing        0.140708    \nas.factor(EducationField)Medical          0.316139    \nas.factor(EducationField)Other            0.177169    \nas.factor(EducationField)Technical Degree 0.367754    \nas.factor(EducationField)Test             0.722669    \nage                                       6.42e-06 ***\nGenderMale                                5.27e-07 ***\nJobLevel                                  0.000933 ***\nMonthlyIncome                             0.001472 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.1 on 23477 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.00395,   Adjusted R-squared:  0.003526 \nF-statistic:  9.31 on 10 and 23477 DF,  p-value: 1.403e-15\n\n\n\n\n\nLogistic Regression\nLogistic regression is used when the dependent variable is binary. We transform Gender into a binary variable and predict it based on MonthlyIncome and JobLevel.\n\n# Convert Gender to binary variable (Male = 1, Female = 0)\ndf &lt;- df %&gt;%\n  mutate(Gender = ifelse(Gender == \"Male\", 1, 0))\n\n# Logistic regression model\nlogit &lt;- glm(Gender ~ MonthlyIncome + JobLevel, data = df, family = \"binomial\")\n\n# Display the summary of the logistic regression model\nsummary(logit)\n\n\nCall:\nglm(formula = Gender ~ MonthlyIncome + JobLevel, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    5.965e-01  3.214e-02  18.558  &lt; 2e-16 ***\nMonthlyIncome  2.086e-05  8.476e-06   2.461   0.0138 *  \nJobLevel      -1.575e-01  3.611e-02  -4.363 1.28e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 31633  on 23503  degrees of freedom\nResidual deviance: 31589  on 23501  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 31595\n\nNumber of Fisher Scoring iterations: 4\n\n# Exponentiate the coefficients to get odds ratios\nexp(coef(logit))\n\n  (Intercept) MonthlyIncome      JobLevel \n    1.8157319     1.0000209     0.8542492"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-cleaning",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-cleaning",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #1 (Cleaning)",
    "text": "Tutorial #1 (Cleaning)\n\nGetting Started\nBefore diving into the transformations, let’s load the necessary package and dataset:\n\n# Uncomment the next four lines if you haven't installed the package\n# install.packages(\"tidyverse\")\n# install.packages(\"psych\")\n# install.packages(\"lsr\")\n# install.packages(\"apaTables\")\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(lsr)\nlibrary(apaTables)\n\n# Loading the dataset\ndf &lt;- read_csv(\"IBM HR Data.csv\")\n\n# Checking all the column names\nnames(df)\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"Application ID\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n[37] \"Employee Source\"         \n\n\n\n\n\nSorting Data\nSorting is often one of the first transformations you’ll perform. Here’s how to do it in base R and tidyverse.\n\nOption 1: Base R\nIn base R, you can use the order() function to sort by one or more columns:\n\n# Sort by age in ascending order\nindex &lt;- order(df$Age) \ndf &lt;- df[index, ]\n\n\n\nOption 2: Tidyverse\nThe arrange() function from dplyr is a more concise and readable option:\n\n# Sort by multiple columns: Age (descending) and DailyRate (ascending)\ndf &lt;- arrange(df, desc(Age), DailyRate)\n\n\n\n\n\nRenaming Columns\nRenaming columns is another frequent task, especially when preparing data for analysis or visualization.\n\nOption 1: Base R\nUse the colnames() function to rename specific columns:\n\n# Rename the first column (age)\ncolnames(df)[1] &lt;- \"how_old\"\n\n\n\nOption 2: Tidyverse\nThe rename() function provides an intuitive way to rename columns:\n\n# Rename \"how_old\" back to \"age\"\ndf &lt;- rename(df, \"age\" = \"how_old\")\n\n\n\n\n\nFiltering Data\nFiltering rows based on specific conditions is crucial for focusing your analysis. Let’s filter for single employees earning over $10,000 per month:\n\n# Filter rows where MaritalStatus is \"Single\" and MonthlyIncome &gt; 10000\nsingle_rich &lt;- df %&gt;%\n  filter(MaritalStatus == \"Single\", MonthlyIncome &gt; 10000)\n\nThe pipe operator (%&gt;%) from the tidyverse makes it easy to chain multiple operations.\n\n\n\nSummarizing Data\nSummarization provides insights into key metrics like mean and standard deviation. Here’s how to summarize the filtered data:\n\n# summarizing the subset data\nsingle_rich %&gt;%\n  summarise(Mean = mean(MonthlyIncome), SD = sd(MonthlyIncome))\n\n# A tibble: 1 × 2\n    Mean    SD\n   &lt;dbl&gt; &lt;dbl&gt;\n1 15043. 3200.\n\n\nThis calculates the average (Mean) and standard deviation (SD) of the MonthlyIncome column for the filtered subset.\n\n\n\nSubsetting Columns\nSelecting specific columns is often necessary to focus on relevant data or remove unnecessary variables. You can use the select() function from the tidyverse to keep only the desired columns:\n\n# Keep specific columns\ndf %&gt;%\n  select(Attrition, DailyRate, Department, Education)\n\n# A tibble: 23,532 × 4\n   Attrition        DailyRate Department             Education\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 Current employee       370 Research & Development         4\n 2 Current employee       370 Research & Development         4\n 3 Current employee       370 Research & Development         4\n 4 Current employee       370 Research & Development         4\n 5 Current employee       370 Research & Development         4\n 6 Current employee       370 Research & Development         4\n 7 Current employee       370 Research & Development         4\n 8 Current employee       370 Research & Development         4\n 9 Current employee       370 Research & Development         4\n10 Current employee       370 Research & Development         4\n# ℹ 23,522 more rows\n\n\nThis selects only the Attrition, DailyRate, Department, and Education columns.\nTo drop specific columns, use the select() function with the negation operator !:\n\n# Drop unnecessary columns\ndf &lt;- df %&gt;%\n  select(!c(EmployeeCount, StandardHours, Over18))\n\nThis removes the EmployeeCount, StandardHours, and Over18 columns.\n\n\nGrouping Data by Variable\nGrouping rows is helpful for summarizing data by categories. Here, we’ll group the data by EducationField and count the number of occurrences in each category.\n\n# Group by EducationField and count rows\ndf %&gt;%\n  group_by(EducationField) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 × 2\n# Groups:   EducationField [9]\n  EducationField       n\n  &lt;chr&gt;            &lt;int&gt;\n1 Life Sciences     9725\n2 Medical           7393\n3 Marketing         2541\n4 Technical Degree  2105\n5 Other             1311\n6 Human Resources    446\n7 &lt;NA&gt;                 9\n8 3                    1\n9 Test                 1\n\n\nThis code:\n\nGroups the data by EducationField.\nCounts the rows in each group.\nArranges the groups in descending order by count (n).\n\n\n\nRecoding Variables\nRecoding variables is a common task when transforming categorical data into numerical or more meaningful labels. First, let’s check the frequency of each category in the Attrition column:\n\n# View the distribution of Attrition\ntable(df$Attrition)\n\n\n     Current employee           Termination Voluntary Resignation \n                19714                    96                  3709 \n\n\nYou can use the recode() function from the tidyverse to assign new values to the categories:\n\n# Recode Attrition variable\ndf$Attrition &lt;- recode(df$Attrition,\n                       \"Current employee\" = 0,\n                       \"Termination\" = 1,\n                       \"Voluntary Resignation\" = 2)\n\nThis transforms Attrition into a numerical variable with the following mapping:\n\n0: Current employee\n1: Termination\n2: Voluntary Resignation"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visualization",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visualization",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #2 (Visualization)",
    "text": "Tutorial #2 (Visualization)\nWe will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere’s how to create a simple scatterplot of YearsAtCompany vs. TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we’ll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere’s what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-4-reports",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-4-reports",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #4 (Reports)",
    "text": "Tutorial #4 (Reports)\nWe will use the apaTables package to format analysis outputs according to APA guidelines and export them to Word documents.\n\nAPA Style Correlation Table\nFirst, we create a correlation matrix using the cor() function for a subset of numeric columns from the dataset.\n\n# Subset of numeric columns for correlation analysis\nnum &lt;- df[, c(\"age\", \"DailyRate\", \"DistanceFromHome\", \n              \"HourlyRate\", \"MonthlyIncome\")]\n\n# Calculate the Pearson correlation matrix\ncorr_matrix &lt;- cor(num, method = \"pearson\", use = \"complete.obs\")\n\nNext, we use the apa.cor.table() function from the apaTables package to create a formatted APA-style table.\n\n# Create and save the APA correlation table\napa.cor.table(corr_matrix, table.number = 1, filename = \"Correlation_table.doc\")\n\n\n\nTable 1 \n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable            M    SD   1           2           3           4          \n  1. age              0.25 0.43                                                \n                                                                               \n  2. DailyRate        0.20 0.45 -.30                                           \n                                [-.94, .79]                                    \n                                                                               \n  3. DistanceFromHome 0.20 0.45 -.33        -.27                               \n                                [-.94, .78] [-.93, .80]                        \n                                                                               \n  4. HourlyRate       0.20 0.44 -.35        -.20        -.21                   \n                                [-.94, .77] [-.92, .83] [-.92, .83]            \n                                                                               \n  5. MonthlyIncome    0.24 0.44 .24         -.35        -.32        -.35       \n                                [-.81, .93] [-.94, .77] [-.94, .78] [-.94, .77]\n                                                                               \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p &lt; .05. ** indicates p &lt; .01.\n \n\n\nThe above code will generate a Word document with the correlation table in APA format, and the file will be saved as “Correlation_table.doc”.\n\n\n\nAPA Style ANOVA Table\nWe will use the apa.aov.table() function to format the ANOVA results into an APA-style table and save it to a Word document.\n\n# Create and save the APA ANOVA table\napa.aov.table(anovaTest, table.number = 2, filename = \"Anova_table.doc\")\n\n\n\nTable 2 \n\nANOVA results using df$JobSatisfaction as the dependent variable\n \n\n     Predictor       SS    df      MS       F    p partial_eta2\n   (Intercept)  7439.59     1 7439.59 6128.95 .000             \n df$Department     0.73     2    0.36    0.30 .742          .00\n         Error 28524.14 23499    1.21                          \n CI_90_partial_eta2\n                   \n         [.00, .00]\n                   \n\nNote: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared \n\n\nThis will generate a Word document containing the APA-formatted ANOVA table, saved as “Anova_table.doc”."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#conclusion",
    "href": "posts/2024-12-31-R-Workshop/index.html#conclusion",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we covered key techniques for data manipulation, analysis, and reporting in R. We started with basic operations such as sorting, filtering, and renaming columns using both base R and the dplyr package. We then explored advanced visualization with ggplot2, creating customized scatterplots and faceted plots. The analysis section demonstrated t-tests, ANOVA, regression models, and post-hoc tests to analyze relationships in the data.\nWe also learned how to create APA-style tables for correlation matrices and ANOVA results using the apaTables package. While we’ve only scratched the surface of what we can do with R, these tutorials should provide a robust foundation for conducting data analysis in research, business, and academic contexts, while hopefully mitigating initial anxieties associated with learning to code. Happy coding!"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visuals",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visuals",
    "title": "R You Ready? An I/O Psychologist’s Guide to R & Rstudio",
    "section": "Tutorial #2 (Visuals)",
    "text": "Tutorial #2 (Visuals)\nWe will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere’s how to create a simple scatterplot of YearsAtCompany vs. TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we’ll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere’s what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  }
]