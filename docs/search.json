[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "What Are Data Structures? A Simple Introduction for Beginners\n\n\n\n\n\n\n\nData Science\n\n\n\n\nLearn what data structures are, why they matter, and how they can improve the efficiency of your code. This blog introduces key concepts in an easy-to-understand way for those just starting their coding journey.\n\n\n\n\n\n\nDec 20, 2024\n\n\nMd Allama Ikbal Sijan\n\n\n\n\n\n\n  \n\n\n\n\nFrom Text to Predictions: Using LLMs for NLP Tasks\n\n\n\n\n\n\n\nLLM\n\n\nNLP\n\n\n\n\nThis blog post walks you through using the transformers library to import pre-trained large language models (LLM) and its tokenizer. You‚Äôll learn how to tokenize text, extract word embeddings from different hidden layers, and use these embeddings in machine learning models for prediction.\n\n\n\n\n\n\nMay 21, 2024\n\n\nMd Allama Ikbal Sijan\n\n\n\n\n\n\n  \n\n\n\n\nR You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio\n\n\n\n\n\n\n\nR\n\n\nWorkshop\n\n\n\n\nMy January 16, 2024 METRO workshop is now a blog post, covering the essential 20% of R syntax for 80% of data analysis tasks. This blog post is designed for both R beginners and those seeking a refresher.\n\n\n\n\n\n\nJan 15, 2024\n\n\nMd Allama Ikbal Sijan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Md Allama Ikbal Sijan",
    "section": "",
    "text": "Hi there! I‚Äôm Sijan (Yess! I go by my last name-I will tell you the story someday!). I‚Äôm a Ph.D.¬†candidate in Industrial-Organizational Psychology with a concentration in Data Science. I also teach undergraduate statistics and have a passion for discussing topics like assessment development, fairness, validity, psychometrics, data visualization, natural language processing (NLP), and artificial intelligence (AI). Poke around my website to learn more!\n\n \n\nwidget credit: richpauloo.github.io"
  },
  {
    "objectID": "posts/2024-12-20-data-structure-basics/index.html",
    "href": "posts/2024-12-20-data-structure-basics/index.html",
    "title": "What Are Data Structures? A Simple Introduction for Beginners",
    "section": "",
    "text": "When working with data, organizing, storing, and performing operations on it efficiently is key. This is where data structures come into play. In simple terms, a data structure is a way of organizing and managing data so that it can be accessed and modified efficiently.\n\nüîÑ Data Type vs.¬†Data Structure\nIf you are reading this, you‚Äôre likely already familiar with data types such as int (integer), float (floating-point number), str (string), and bool (Boolean). These are the building blocks used to define the kind of data you‚Äôre working with. When you perform data cleaning or analysis, you often manipulate these primitive data types.\nData structures build on these basic data types to manage and organize data more effectively. Think of it like constructing a car: the raw materials like metal and plastic are akin to the primitive data types, while the car‚Äôs components, such as the engine, trunk, doors, and wheels, represent the data structures. In essence, data structures use data types to create organized frameworks that allow for efficient storage and operations on your data.\n\n\n‚öôÔ∏è Common Operations\nData structures are essential tools in programming because they allow you to store, organize, and manage data efficiently. Common operations performed with data structures include:\n\nInsertion: Adding new elements.\nDeletion: Removing existing elements.\nSearching: Finding specific elements.\nAccessing: Retrieving elements by their position or key.\nUpdating: Modifying existing elements.\n\nWhile data types tell the computer what kind of data is being stored (e.g., integers, strings, floats), they do not inherently provide a way to store multiple values together or perform advanced operations. For managing multiple similar data points, data structures become invaluable.\n\n\nüí° Why Use Data Structures?\nIf you have several similar pieces of data to store and manipulate later, using a data structure allows for organized and efficient operations. Here‚Äôs an example:\n\n\nExample: Storing Student Grades\nImagine you have 5 students in a class who took a final exam. Their grades need to be stored for future calculations.\n\nWithout a Data Structure\nUsing only data types, you could store the grades in separate variables like this:\nstudent1 = 85\nstudent2 = 90\nstudent3 = 78\nstudent4 = 92\nstudent5 = 88\nThis approach works but becomes cumbersome as the number of students increases. Operations like calculating the average grade or updating a grade require manual effort for each variable.\n\n\nWith a Data Structure (Array)\nInstead, you can use an array to store the grades:\nstudent_grades = [85, 90, 78, 92, 88]\nNow you can perform operations more efficiently:\n# Access a specific grade: Retrieve the second student's grade\nsecond_grade = student_grades[1]  # Outputs 90\n\n# Update a grade: Add extra credit to the third student's grade\nstudent_grades[2] += 5  # Updates 78 to 83\n\n# Calculate the average grade: Compute the average for all students\naverage_grade = sum(student_grades) / len(student_grades)  # Outputs 86.6\n\n# Add a new student: Include another student's grade\nstudent_grades.append(95)\n\n# Remove a student: Remove the grade of a student who dropped out\nstudent_grades.pop(1)  # Removes the second student's grade\n\n\nWith a Data Structure (Hash Table)\nIf you need to associate grades with student names for better clarity, a hash table (dictionary) works even better:\nstudent_grades = {\n    \"Sijan\": 85,\n    \"Bella\": 90,\n    \"Alex\": 78,\n    \"Taylor\": 92,\n    \"Jordan\": 88\n}\nWith this structure:\n\nAccessing a grade by name is straightforward:\n\nbella_grade = student_grades[\"Bella\"]  # Outputs 90\n\nUpdating grades is intuitive:\n\nstudent_grades[\"Alex\"] += 5  # Updates Alex's grade to 83\n\nAdding or removing students is easy:\n\nstudent_grades[\"Chris\"] = 95  # Adds Chris with a grade of 95\ndel student_grades[\"Taylor\"]  # Removes Taylor\n\n\n\n\nüõ†Ô∏è Common Data Structures\nUnderstanding the unique characteristics of different data structures‚Äîwhether they are ordered or unordered, and what operations they excel at‚Äîhelps in selecting the best one for a given task.\n\nArray: An ordered collection of elements, best for quick access and iteration.\nLinked List: An ordered, dynamic structure where elements are linked, ideal for frequent insertions and deletions.\nTrees: A hierarchical, ordered structure, excellent for representing hierarchical relationships and performing searches.\nHash Table: An unordered structure using key-value pairs, best for fast lookups and retrievals.\nHeap: A specialized tree-based structure, great for efficient retrieval of the smallest or largest element.\nGraphs: A network of nodes connected by edges, perfect for modeling relationships and performing pathfinding operations.\n\n\n\nüîç Data Structures vs.¬†Abstract Data Types (ADT)\nEven if you‚Äôre not very familiar with the different data structures I‚Äôve mentioned, you‚Äôre likely familiar with Abstract Data Types (ADTs) like lists, sets, and dictionaries. ADTs essentially define the operations or functionalities you can perform on data, while data structures are the low-level implementations that make those operations possible. Using the car example again, the ADT represents the car‚Äôs functionalities‚Äîdriving, braking, and steering‚Äîwhile the data structures are the specific building blocks like the engine, wheels, and frame that make those functionalities work. ADTs are high-level abstract concepts, while data structures are the concrete details that enable those concepts to function.\nKnowing just the ADT and its functionality will take you a long way, and you don‚Äôt necessarily need to understand the underlying data structures to perform different tasks. For instance, a list can be implemented using either an array or a linked list. When you use the built-in list function, like my_list = [], you don‚Äôt need to know whether it‚Äôs implemented using an array or a linked list. However, understanding the underlying data structure can be important for optimizing performance, such as improving runtime or space complexity. This deeper understanding helps you make more efficient decisions when choosing how to implement solutions, especially when performance is critical.\n\n\nüóÇÔ∏è Common ADTs\nHere are some common ADTs and the data structures used to implement them:\n\nList: Array, Linked List\nStack: Array, Linked List\nQueue: Array, Linked List, Circular Buffer\nDeque: Array, Doubly Linked List\nSet: Hash Table, BST\nBag: Array, Linked List\nPriority Queue: Binary Heap, Fibonacci Heap\nDictionary: Hash Table, BST\n\n\n\nüåç Real World Applications\nDifferent data structures are utilized in real-world applications to optimize performance, manage data efficiently, and solve complex problems. Here are some practical implementations of commonly used data structures:\n\nArray: Arrays are used in applications like image processing and spreadsheets where data is stored in contiguous memory locations for quick access.\nLinked List: Linked lists are utilized in web browsers to manage navigation history, allowing for easy forward and backward movement.\nStack: Stacks are implemented in function call management, as well as in undo/redo operations in text editors, following the Last In, First Out (LIFO) principle.\nQueue: Queues are used in operating systems for task scheduling and in customer service systems to process requests in the order they arrive.\nSet: Sets are applied in social media platforms to store unique user data, such as friend lists, without duplicates.\nHash Table: Hash tables are used in contact management systems to map a user‚Äôs name to their phone number for fast lookups.\nBinary Search Tree (BST): BSTs are used in databases for fast searching, insertion, and deletion of sorted records.\nHeap: Heaps are implemented in job scheduling systems to prioritize tasks based on urgency or importance.\nGraph: Graphs are used to model networks like social media connections, transportation systems, and web pages.\n\n\n\nüí≠ Conclusion\nThese are just a few examples of how data structures are applied in the real world, and there are countless other uses that I haven‚Äôt covered here. It took me some time to truly grasp the key differences between data structures and other data types. This brief introduction to data structures serves as a way for me to reinforce my own understanding and, hopefully, help others in the process.\nData structures are fundamental tools for organizing and managing data efficiently across various real-world applications. From simple structures like arrays and linked lists to more complex ones like hash tables and graphs, each data structure has its own purpose based on the specific problem you‚Äôre trying to solve. Understanding these practical applications enables us to select the right data structure, optimize performance, and tackle challenges more effectively.\nIf you found this post helpful or have any feedback, I‚Äôd love to hear your thoughts‚Äîfeel free to leave a comment below!\n\n\nüîó Reference\nzyBooks, a Wiley brand. (2024). Data Structures With Python. https://learn.zybooks.com/zybook/MONTCLAIRCSIT506ZharriFall2024 (accessed 2024).\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{allama ikbal sijan2024,\n  author = {Allama Ikbal Sijan, Md},\n  title = {What {Are} {Data} {Structures?} {A} {Simple} {Introduction}\n    for {Beginners}},\n  date = {2024-12-20},\n  url = {https://sijan14.github.io/sijan_portfolio/posts/2024-12-20-data-structure-basics/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAllama Ikbal Sijan, Md. 2024. ‚ÄúWhat Are Data Structures? A Simple\nIntroduction for Beginners.‚Äù December 20, 2024. https://sijan14.github.io/sijan_portfolio/posts/2024-12-20-data-structure-basics/."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "üï∞Ô∏è Delay Discounting as a Latent Factor\n\nDate: December 15, 2024\n\nThis project examines the latent factor structure of delay discounting, the tendency to prioritize immediate rewards over delayed ones, which is linked to behavioral outcomes such as substance abuse gambling, credit card debt, and poor academic performance (W√∂lfling et al., 2020). Previous studies have used various methods to measure delay discounting, but the findings have been inconsistent, partly due to differences in operationalization. This study uses confirmatory factor analysis (CFA) to explore the underlying latent factors of delay discounting and their relationship to behavioral outcomes, providing a clearer understanding of the construct and its implications.\n\n\n\n\n\n\nOne Factor Model:\n\n\nCFI = .72\n\n\nRMSEA = .24\n\n\nSRMR = .109\n\n\nAvg R2 = .60\n\n\n\n\n\nTwo Factor Model:\n\n\nCFI = .94\n\n\nRMSEA = .12\n\n\nSRMR = .04\n\n\nAvg R2 = .69\n\n\n\n\n\nFour Factor Model:\n\n\nCFI = .96\n\n\nRMSEA = .10\n\n\nSRMR = .04\n\n\nAvg R2 = .69\n\n\n\n\n\n\n\n\n\nü§ñ Which ML Algorithms Predict Job Satisfaction The Best?\n\nDate: May 2, 2023\n\nMachine learning algorithms have gained significant popularity in I/O psychology due to their advanced learning capabilities, often outperforming traditional regression methods in predictive tasks. However, their ‚Äúblack-box‚Äù nature remains a challenge for research justification. This project compares the performance of baseline model logistic regression with popular algorithms KNN, and random forest in a 4-class job satisfaction classification task using the IBM HR dataset from Kaggle, comprising approximately 23,000 observations. Using lasso-based feature-selection methods, hyperparameter tuning, the project optimizes model performance and identifies the algorithm with the highest predictive accuracy. The findings offer actionable insights into employee well-being, showcasing the potential of data-driven approaches to enhance workforce engagement and organizational performance.\n\n\n\n\n\n\n\n\n\n\n\n\n‚öñÔ∏è Conscientiousness Scale: Development & Validation\n\nDate: April 21, 2023\n\nThis project involved the psychometric development of a new Conscientiousness scale, one of the Big Five personality traits. Following best-practice item-writing guidelines, I conducted a pilot study and refined the item pool by removing items with low item-total correlations and minimal impact on Cronbach‚Äôs alpha if removed (see Figure 1 & 2). Subsequent analyses demonstrated strong internal consistency (Œ± = .91) and validity evidence. The new scale exhibited high convergent validity (r = .85) with the well-validated IPIP Conscientiousness scale and good discriminant validity with other Big Five dimensions (see Figure 3). Criterion validity was supported by a positive correlation with job performance (r = .33), consistent with meta-analytic findings (Sackett et al., 2022), establishing the scale as a valid measure of conscientiousness.\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey there! I‚Äôm Sijan, a Ph.D.¬†candidate in Industrial-Organizational Psychology with a concentration in Data Science at Montclair State University. In other words, I‚Äôm passionate about people, data, and making workplaces better for everyone.\n\n\nI blend psychology with cutting-edge data science techniques to create psychometrically valid, reliable assessment tools. By leveraging natural language processing (NLP) and large language models (LLMs), I design psychometrically robust tools to predict key organizational metrics like job satisfaction, engagement, and turnover. My most recent work focuses on examining the validity, predictive bias, and risk of adverse impact of AI-based assessment tools and scoring systems. I also explore topics like team decision-making, delay discounting, and biases and heuristics.\n\n\n\nWhen I‚Äôm not diving into data or teaching statistics as an adjunct professor, I‚Äôm leading initiatives and workshops for METRO, SIO NYC, and Eagle I/O, empowering I/O students and professionals. I‚Äôve also worked with organizations like JUST Capital and City Bank, helping them make data-driven decisions to enhance their workforce strategies."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hey there! I‚Äôm Sijan, a Ph.D.¬†candidate in Industrial-Organizational Psychology with a concentration in Data Science at Montclair State University. In other words, I‚Äôm passionate about people, data, and making workplaces better for everyone.\n\n\nI blend psychology with cutting-edge data science techniques to create psychometrically valid, reliable assessment tools. By leveraging natural language processing (NLP) and large language models (LLMs), I design psychometrically robust tools to predict key organizational metrics like job satisfaction, engagement, and turnover. My most recent work focuses on examining the validity, predictive bias, and risk of adverse impact of AI-based assessment tools and scoring systems. I also explore topics like team decision-making, delay discounting, and biases and heuristics.\n\n\n\nWhen I‚Äôm not diving into data or teaching statistics as an adjunct professor, I‚Äôm leading initiatives and workshops for METRO, SIO NYC, and Eagle I/O, empowering I/O students and professionals. I‚Äôve also worked with organizations like JUST Capital and City Bank, helping them make data-driven decisions to enhance their workforce strategies."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "üéì Education",
    "text": "üéì Education\n\nDoctor of Philosophy, Industrial-Organizational Psychology (Concentration: Data Science)\nMontclair State University | 2022 - 2026\nGraduate Certificate in Advanced Quantitative Methods\nMontclair State University | 2022 - 2025\nMaster of Arts, Industrial-Organizational Psychology\nMontclair State University | 2022 - 2024\nBachelor of Arts, Global Management & Psychology\nEarlham College | 2018 - 2022"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "üßëüèΩ‚Äçüíª Experience",
    "text": "üßëüèΩ‚Äçüíª Experience\n\nDoctoral Research Assistant, Montclair State University | August 2022 - Present\nAdjunct Professor, Montclair State University | September 2024 - Present\nCo-Director of Education, METRO | October 2023 - Present\nFounding Board Member, SIONYC | September 2023 - May 2024\nPresident & Consultant, Eagle I.O | September 2023 - May 2024\nResearch Intern, JUST Capital | May 2023 - August 2023\nGraduate Assistant, Center for Writing Excellence | September 2022 - May 2023\nConsulting & Data Analytics Extern, Paragon One | June 2022 - September 2022\nHR Reporting Intern, City Bank Limited | May 2021 - August 2021"
  },
  {
    "objectID": "about.html#presentations",
    "href": "about.html#presentations",
    "title": "About",
    "section": "üéôÔ∏èPresentations",
    "text": "üéôÔ∏èPresentations\n\nSijan, M. (Co-chair), Belwalkar, B. B. (Co-chair) & Mracek, D. (Discussant) (2025). Power of NLP and LLMs: Turning I/O Research and Practice into a Textual Adventure [Symposium]. Society for Industrial and Organizational Psychology Annual Conference, Denver, CO, United States.*\nHuynh, C., Elfeki, Y., Sijan, M. (2025). Modeling Latent Constructs in SJT Using Pseudo-Factor Analysis [Poster]. Society for Industrial and Organizational Psychology Annual Conference, Denver, CO, United States.*\nSijan, M., Bixter, M. T. (2024). Using NLP to Predict Job Satisfaction and Turnover Intention [Poster Presentation]. The Psychonomic Society Annual Meeting, New York, NY, United States.\nBragger, J., Helisch, R., Buczek, E., Hunt, L., Sijan, M., Abbasi, F., Filstein, T., Myszko, Z., Sherman, C., & Lakusta, L. (2024). Neurodiversity in Leadership: The Relationship Between Theory of Mind and Leadership Roles, and Possible Implications for Neurodivergent Workers [Presentation]. Interdisciplinary Perspectives on Leadership Symposium, Thessaloniki, Greece.\nSijan, M., Kramer, M. (Co-Chair), Kulas, J. (Co-Chair), Helisch, R., Lancuna, L., Defabiis, M., Hunt, L., Notari, K., (2024). Unlocking the value of IO Psychology Student Consulting in Higher Education [Panel Presentation]. Society for Industrial and Organizational Psychology Annual Conference, Chicago, IL, United States.\nElfeki, Y., Huynh, C., Sijan, M., Salter, N. P. (2024). An Exploration of Masculinity and Femininity Perceptions Across Monoracial and Biracial Identities using StyleGAN Imagery [Symposium]. Society for Industrial and Organizational Psychology Annual Conference, Chicago, IL, United States.\nHunt, L., Bragger, J., Kubu, G., Abbasi, F., & Sijan, M. (2024). Leading the Goals‚Äô Rush: How leadership mindsets make all the difference [Poster Presentation]. Society for Industrial and Organizational Psychology Annual Conference, Chicago, IL, United States.\nSijan M. (2022). Perception of Trust Based on Parenthood Status [Poster Presentation]. 94th Annual MPA (Midwestern Psychological Association) Conference. Chicago, IL, United States."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html",
    "href": "posts/2024-12-31-R-Workshop/index.html",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "",
    "text": "In recent years, the R programming language emerged as the industry standard in social science research, owing to its robust statistical analysis tools, powerful data visualization capabilities, and a rich ecosystem of packages. Its open-source nature and emphasis on reproducibility further propelled its widespread adoption. This workshop, titled ‚ÄúR You Ready? An I/O Psychologist‚Äôs Guide to R and RStudio,‚Äù was tailored for both R novices and those seeking a refresher, focusing on the essential 20% of R syntax that facilitated 80% of data analysis tasks. Participants were introduced to the RStudio environment and fundamental R syntax, followed by hands-on sessions covering data transformation (dplyr), data visualization (ggplot2), statistical analyses (lm, stats), APA-style reporting (markdown, apa, papaja), and text mining (stringr, tidytext). The workshop concluded with collaborative problem-solving sessions to reinforce the concepts learned. The aim was to introduce students and practitioners to the power of R to carry out basic and advanced analyses, along with the capability to visualize and report findings."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-1",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-1",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "Tutorial #1",
    "text": "Tutorial #1\n\nGetting Started\nBefore diving into the transformations, let‚Äôs load the necessary package and dataset:\n\n# Uncomment the next line if you haven't installed the package\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# loading the dataset\ndf &lt;- read_csv(\"IBM HR Data.csv\")\n\n# checking all the column names\nnames(df)\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"Application ID\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n[37] \"Employee Source\"         \n\n\n\n\n\nSorting Data\nSorting is often one of the first transformations you‚Äôll perform. Here‚Äôs how to do it in base R and tidyverse.\n\nOption 1: Base R\nIn base R, you can use the order() function to sort by one or more columns:\n\n# sort by age in ascending order\nindex &lt;- order(df$Age) \ndf &lt;- df[index, ]\n\n\n\nOption 2: Tidyverse\nThe arrange() function from dplyr is a more concise and readable option:\n\n# Sort by multiple columns: Age (descending) and DailyRate (ascending)\ndf &lt;- arrange(df, desc(Age), DailyRate)\n\n\n\n\n\nRenaming Columns\nRenaming columns is another frequent task, especially when preparing data for analysis or visualization.\n\nOption 1: Base R\nUse the colnames() function to rename specific columns:\n\n# Rename the first column (age)\ncolnames(df)[1] &lt;- \"how_old\"\n\n\n\nOption 2: Tidyverse\nThe rename() function provides an intuitive way to rename columns:\n\n# Rename \"how_old\" back to \"age\"\ndf &lt;- rename(df, \"age\" = \"how_old\")\n\n\n\n\n\nFiltering Data\nFiltering rows based on specific conditions is crucial for focusing your analysis. Let‚Äôs filter for single employees earning over $10,000 per month:\n\n# Filter rows where MaritalStatus is \"Single\" and MonthlyIncome &gt; 10000\nsingle_rich &lt;- df %&gt;%\n  filter(MaritalStatus == \"Single\", MonthlyIncome &gt; 10000)\n\nThe pipe operator (%&gt;%) from the tidyverse makes it easy to chain multiple operations.\n\n\n\nSummarizing Data\nSummarization provides insights into key metrics like mean and standard deviation. Here‚Äôs how to summarize the filtered data:\n\n# summarizing the subset data\nsingle_rich %&gt;%\n  summarise(Mean = mean(MonthlyIncome), SD = sd(MonthlyIncome))\n\n# A tibble: 1 √ó 2\n    Mean    SD\n   &lt;dbl&gt; &lt;dbl&gt;\n1 15043. 3200.\n\n\nThis calculates the average (Mean) and standard deviation (SD) of the MonthlyIncome column for the filtered subset.\n\n\n\nSubsetting Columns\nSelecting specific columns is often necessary to focus on relevant data or remove unnecessary variables. You can use the select() function from the tidyverse to keep only the desired columns:\n\n# Keep specific columns\ndf %&gt;%\n  select(Attrition, DailyRate, Department, Education)\n\n# A tibble: 23,532 √ó 4\n   Attrition        DailyRate Department             Education\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 Current employee       370 Research & Development         4\n 2 Current employee       370 Research & Development         4\n 3 Current employee       370 Research & Development         4\n 4 Current employee       370 Research & Development         4\n 5 Current employee       370 Research & Development         4\n 6 Current employee       370 Research & Development         4\n 7 Current employee       370 Research & Development         4\n 8 Current employee       370 Research & Development         4\n 9 Current employee       370 Research & Development         4\n10 Current employee       370 Research & Development         4\n# ‚Ñπ 23,522 more rows\n\n\nThis selects only the Attrition, DailyRate, Department, and Education columns.\nTo drop specific columns, use the select() function with the negation operator !:\n\n# Drop unnecessary columns\ndf &lt;- df %&gt;%\n  select(!c(EmployeeCount, StandardHours, Over18))\n\nThis removes the EmployeeCount, StandardHours, and Over18 columns.\n\n\nGrouping Data by Variable\nGrouping rows is helpful for summarizing data by categories. Here, we‚Äôll group the data by EducationField and count the number of occurrences in each category.\n\n# Group by EducationField and count rows\ndf %&gt;%\n  group_by(EducationField) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 √ó 2\n# Groups:   EducationField [9]\n  EducationField       n\n  &lt;chr&gt;            &lt;int&gt;\n1 Life Sciences     9725\n2 Medical           7393\n3 Marketing         2541\n4 Technical Degree  2105\n5 Other             1311\n6 Human Resources    446\n7 &lt;NA&gt;                 9\n8 3                    1\n9 Test                 1\n\n\nThis code:\n\nGroups the data by EducationField.\nCounts the rows in each group.\nArranges the groups in descending order by count (n).\n\n\n\nRecoding Variables\nRecoding variables is a common task when transforming categorical data into numerical or more meaningful labels. First, let‚Äôs check the frequency of each category in the Attrition column:\n\n# View the distribution of Attrition\ntable(df$Attrition)\n\n\n     Current employee           Termination Voluntary Resignation \n                19714                    96                  3709 \n\n\nYou can use the recode() function from the tidyverse to assign new values to the categories:\n\n# Recode Attrition variable\ndf$Attrition &lt;- recode(df$Attrition,\n                       \"Current employee\" = 0,\n                       \"Termination\" = 1,\n                       \"Voluntary Resignation\" = 2)\n\nThis transforms Attrition into a numerical variable with the following mapping:\n\n0: Current employee\n1: Termination\n2: Voluntary Resignation"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "Tutorial #2",
    "text": "Tutorial #2\nIn this tutorial, we will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere‚Äôs how to create a simple scatterplot of YearsAtCompany vs.¬†TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we‚Äôll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere‚Äôs what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#scatterplots-in-ggplot2",
    "href": "posts/2024-12-31-R-Workshop/index.html#scatterplots-in-ggplot2",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "Scatterplots in ggplot2",
    "text": "Scatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\n\nBasic Scatterplot\nHere‚Äôs how to create a simple scatterplot of YearsAtCompany vs.¬†TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#introduction",
    "href": "posts/2024-12-31-R-Workshop/index.html#introduction",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "",
    "text": "In recent years, the R programming language emerged as the industry standard in social science research, owing to its robust statistical analysis tools, powerful data visualization capabilities, and a rich ecosystem of packages. Its open-source nature and emphasis on reproducibility further propelled its widespread adoption. This workshop, titled ‚ÄúR You Ready? An I/O Psychologist‚Äôs Guide to R and RStudio,‚Äù was tailored for both R novices and those seeking a refresher, focusing on the essential 20% of R syntax that facilitated 80% of data analysis tasks. Participants were introduced to the RStudio environment and fundamental R syntax, followed by hands-on sessions covering data transformation (dplyr), data visualization (ggplot2), statistical analyses (lm, stats), APA-style reporting (markdown, apa, papaja), and text mining (stringr, tidytext). The workshop concluded with collaborative problem-solving sessions to reinforce the concepts learned. The aim was to introduce students and practitioners to the power of R to carry out basic and advanced analyses, along with the capability to visualize and report findings."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-data-cleaning",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-data-cleaning",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "Tutorial #1 (Data Cleaning)",
    "text": "Tutorial #1 (Data Cleaning)\n\nGetting Started\nBefore diving into the transformations, let‚Äôs load the necessary package and dataset:\n\n# Uncomment the next two lines if you haven't installed the package\n# install.packages(\"tidyverse\")\n# install.packages(\"psych\")\n# install.packages(\"lsr\")\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(lsr)\n\n# Loading the dataset\ndf &lt;- read_csv(\"IBM HR Data.csv\")\n\n# Checking all the column names\nnames(df)\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"Application ID\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n[37] \"Employee Source\"         \n\n\n\n\n\nSorting Data\nSorting is often one of the first transformations you‚Äôll perform. Here‚Äôs how to do it in base R and tidyverse.\n\nOption 1: Base R\nIn base R, you can use the order() function to sort by one or more columns:\n\n# Sort by age in ascending order\nindex &lt;- order(df$Age) \ndf &lt;- df[index, ]\n\n\n\nOption 2: Tidyverse\nThe arrange() function from dplyr is a more concise and readable option:\n\n# Sort by multiple columns: Age (descending) and DailyRate (ascending)\ndf &lt;- arrange(df, desc(Age), DailyRate)\n\n\n\n\n\nRenaming Columns\nRenaming columns is another frequent task, especially when preparing data for analysis or visualization.\n\nOption 1: Base R\nUse the colnames() function to rename specific columns:\n\n# Rename the first column (age)\ncolnames(df)[1] &lt;- \"how_old\"\n\n\n\nOption 2: Tidyverse\nThe rename() function provides an intuitive way to rename columns:\n\n# Rename \"how_old\" back to \"age\"\ndf &lt;- rename(df, \"age\" = \"how_old\")\n\n\n\n\n\nFiltering Data\nFiltering rows based on specific conditions is crucial for focusing your analysis. Let‚Äôs filter for single employees earning over $10,000 per month:\n\n# Filter rows where MaritalStatus is \"Single\" and MonthlyIncome &gt; 10000\nsingle_rich &lt;- df %&gt;%\n  filter(MaritalStatus == \"Single\", MonthlyIncome &gt; 10000)\n\nThe pipe operator (%&gt;%) from the tidyverse makes it easy to chain multiple operations.\n\n\n\nSummarizing Data\nSummarization provides insights into key metrics like mean and standard deviation. Here‚Äôs how to summarize the filtered data:\n\n# summarizing the subset data\nsingle_rich %&gt;%\n  summarise(Mean = mean(MonthlyIncome), SD = sd(MonthlyIncome))\n\n# A tibble: 1 √ó 2\n    Mean    SD\n   &lt;dbl&gt; &lt;dbl&gt;\n1 15043. 3200.\n\n\nThis calculates the average (Mean) and standard deviation (SD) of the MonthlyIncome column for the filtered subset.\n\n\n\nSubsetting Columns\nSelecting specific columns is often necessary to focus on relevant data or remove unnecessary variables. You can use the select() function from the tidyverse to keep only the desired columns:\n\n# Keep specific columns\ndf %&gt;%\n  select(Attrition, DailyRate, Department, Education)\n\n# A tibble: 23,532 √ó 4\n   Attrition        DailyRate Department             Education\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 Current employee       370 Research & Development         4\n 2 Current employee       370 Research & Development         4\n 3 Current employee       370 Research & Development         4\n 4 Current employee       370 Research & Development         4\n 5 Current employee       370 Research & Development         4\n 6 Current employee       370 Research & Development         4\n 7 Current employee       370 Research & Development         4\n 8 Current employee       370 Research & Development         4\n 9 Current employee       370 Research & Development         4\n10 Current employee       370 Research & Development         4\n# ‚Ñπ 23,522 more rows\n\n\nThis selects only the Attrition, DailyRate, Department, and Education columns.\nTo drop specific columns, use the select() function with the negation operator !:\n\n# Drop unnecessary columns\ndf &lt;- df %&gt;%\n  select(!c(EmployeeCount, StandardHours, Over18))\n\nThis removes the EmployeeCount, StandardHours, and Over18 columns.\n\n\nGrouping Data by Variable\nGrouping rows is helpful for summarizing data by categories. Here, we‚Äôll group the data by EducationField and count the number of occurrences in each category.\n\n# Group by EducationField and count rows\ndf %&gt;%\n  group_by(EducationField) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 √ó 2\n# Groups:   EducationField [9]\n  EducationField       n\n  &lt;chr&gt;            &lt;int&gt;\n1 Life Sciences     9725\n2 Medical           7393\n3 Marketing         2541\n4 Technical Degree  2105\n5 Other             1311\n6 Human Resources    446\n7 &lt;NA&gt;                 9\n8 3                    1\n9 Test                 1\n\n\nThis code:\n\nGroups the data by EducationField.\nCounts the rows in each group.\nArranges the groups in descending order by count (n).\n\n\n\nRecoding Variables\nRecoding variables is a common task when transforming categorical data into numerical or more meaningful labels. First, let‚Äôs check the frequency of each category in the Attrition column:\n\n# View the distribution of Attrition\ntable(df$Attrition)\n\n\n     Current employee           Termination Voluntary Resignation \n                19714                    96                  3709 \n\n\nYou can use the recode() function from the tidyverse to assign new values to the categories:\n\n# Recode Attrition variable\ndf$Attrition &lt;- recode(df$Attrition,\n                       \"Current employee\" = 0,\n                       \"Termination\" = 1,\n                       \"Voluntary Resignation\" = 2)\n\nThis transforms Attrition into a numerical variable with the following mapping:\n\n0: Current employee\n1: Termination\n2: Voluntary Resignation"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-data-visualization",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-data-visualization",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "Tutorial #2 (Data Visualization)",
    "text": "Tutorial #2 (Data Visualization)\nIn this tutorial, we will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere‚Äôs how to create a simple scatterplot of YearsAtCompany vs.¬†TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we‚Äôll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere‚Äôs what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-3-analysis",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-3-analysis",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "üìà Tutorial #3 (Analysis)",
    "text": "üìà Tutorial #3 (Analysis)\n\nSummary and Frequency Tables\nUse summary() and table() to calculate basic descriptive statistics and frequency counts.\n\n# Descriptive statistics for numeric variables\nsummary(df$WorkLifeBalance)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   2.000   3.000   2.761   3.000   4.000      10 \n\n# Frequency table for categorical variables\ntable(df$Gender)\n\n\n     1      2 Female   Male \n     1      1   9400  14120 \n\n\nUsing the describe function from the psych Package for Detailed Summaries of numeric variables\n\n# Descriptive statistics for MonthlyIncome\ndescribe(df$MonthlyIncome)\n\n   vars     n    mean      sd median trimmed     mad  min   max range skew\nX1    1 23516 6502.76 4705.99   4936 5666.19 3278.03 1009 19999 18990 1.37\n   kurtosis    se\nX1        1 30.69\n\n\n\n\n\nt-tests\nA t-test is used to compare the means of two groups. Here, we examine differences in JobSatisfaction between genders.\n\n# Filtering Gender\ndf &lt;- df %&gt;%\n  filter(Gender %in% c(\"Female\", \"Male\"))\n\n# Perform t-test\nt.test(df$JobSatisfaction ~ df$Gender, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  df$JobSatisfaction by df$Gender\nt = -5.0624, df = 23511, p-value = 4.171e-07\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.10296313 -0.04548644\nsample estimates:\nmean in group Female   mean in group Male \n            2.681247             2.755472 \n\n\n\n\nEffect Size\nEffect size provides a measure of the magnitude of differences. We calculate Cohen‚Äôs D using the lsr package.\n\n# Cohen's D for JobSatisfaction by Gender\ncohensD(df$JobSatisfaction ~ df$Gender)\n\n[1] 0.0674014\n\n\n\n\n\n\nOne-Way ANOVA\nANOVA (Analysis of Variance) is used to compare the means of more than two groups. We analyze JobSatisfaction across Department.\n\n# Convert Department to factor\nclass(df$Department) # Check class\n\n[1] \"character\"\n\ndf$Department &lt;- as.factor(df$Department) # Convert to factor\n\n# Conduct ANOVA\nanovaTest &lt;- aov(df$JobSatisfaction ~ df$Department)\nsummary(anovaTest)\n\n                 Df Sum Sq Mean Sq F value Pr(&gt;F)\ndf$Department     2      1   0.363   0.299  0.742\nResiduals     23499  28524   1.214               \n18 observations deleted due to missingness\n\n\n\nEffect Size\nafter conducting the ANOVA, we apply Tukey‚Äôs HSD to compare the JobSatisfaction between different levels of Department.\n\n# Post-hoc Tukey's HSD test\nTukeyHSD(anovaTest)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = df$JobSatisfaction ~ df$Department)\n\n$`df$Department`\n                                              diff         lwr        upr\nResearch & Development-Human Resources 0.021787253 -0.06179058 0.10536508\nSales-Human Resources                  0.027838399 -0.05866972 0.11434652\nSales-Research & Development           0.006051146 -0.03093437 0.04303666\n                                           p adj\nResearch & Development-Human Resources 0.8141109\nSales-Human Resources                  0.7310521\nSales-Research & Development           0.9221490\n\n\n\n\n\n\nCorrelation Analysis\nThe correlation between two continuous variables can be calculated using the cor() or cor.test function. Here, we calculate the correlation between JobInvolvement and RelationshipSatisfaction.\n\n# Calculate correlation, using complete observations only\ncor.test(df$JobInvolvement, df$RelationshipSatisfaction, use = \"complete.obs\")\n\n\n    Pearson's product-moment correlation\n\ndata:  df$JobInvolvement and df$RelationshipSatisfaction\nt = 5.1325, df = 23506, p-value = 2.882e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.02068330 0.04622137\nsample estimates:\n      cor \n0.0334578 \n\n\n\n\n\nMultiple Linear Regression\nA multiple linear regression model can be used to predict the value of a continuous dependent variable based on multiple independent variables. Here, we build a model predicting JobSatisfaction.\n\n# Multiple linear regression model\nmodel1 &lt;- lm(JobSatisfaction ~ \n              age + Gender + JobLevel + MonthlyIncome, \n            data = df)\n\n# Display the summary of the model\nsummary(model1)\n\n\nCall:\nlm(formula = JobSatisfaction ~ age + Gender + JobLevel + MonthlyIncome, \n    data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0440 -0.7585  0.2526  1.2248  1.4585 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.777e+00  3.336e-02  83.237  &lt; 2e-16 ***\nage           -3.597e-03  8.127e-04  -4.426 9.63e-06 ***\nGenderMale     7.321e-02  1.468e-02   4.989 6.13e-07 ***\nJobLevel       6.273e-02  1.948e-02   3.220  0.00128 ** \nMonthlyIncome -1.412e-05  4.568e-06  -3.090  0.00200 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.101 on 23490 degrees of freedom\n  (25 observations deleted due to missingness)\nMultiple R-squared:  0.002353,  Adjusted R-squared:  0.002183 \nF-statistic: 13.85 on 4 and 23490 DF,  p-value: 2.77e-11\n\n# Second model including EducationField as a factor variable\nmodel2 &lt;- lm(JobSatisfaction ~ \n               as.factor(EducationField) + \n               age + Gender + JobLevel + \n               MonthlyIncome, \n             data = df)\n\n# Display the summary of the model\nsummary(model2)\n\n\nCall:\nlm(formula = JobSatisfaction ~ as.factor(EducationField) + age + \n    Gender + JobLevel + MonthlyIncome, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0400 -0.7611  0.2494  1.2052  1.5089 \n\nCoefficients:\n                                            Estimate Std. Error t value\n(Intercept)                                2.683e+00  6.190e-02  43.347\nas.factor(EducationField)Life Sciences     1.459e-01  5.333e-02   2.735\nas.factor(EducationField)Marketing         8.328e-02  5.653e-02   1.473\nas.factor(EducationField)Medical           5.383e-02  5.370e-02   1.002\nas.factor(EducationField)Other             8.145e-02  6.036e-02   1.350\nas.factor(EducationField)Technical Degree  5.170e-02  5.740e-02   0.901\nas.factor(EducationField)Test              3.908e-01  1.101e+00   0.355\nage                                       -3.670e-03  8.132e-04  -4.513\nGenderMale                                 7.362e-02  1.467e-02   5.018\nJobLevel                                   6.450e-02  1.948e-02   3.311\nMonthlyIncome                             -1.453e-05  4.568e-06  -3.180\n                                          Pr(&gt;|t|)    \n(Intercept)                                &lt; 2e-16 ***\nas.factor(EducationField)Life Sciences    0.006240 ** \nas.factor(EducationField)Marketing        0.140708    \nas.factor(EducationField)Medical          0.316139    \nas.factor(EducationField)Other            0.177169    \nas.factor(EducationField)Technical Degree 0.367754    \nas.factor(EducationField)Test             0.722669    \nage                                       6.42e-06 ***\nGenderMale                                5.27e-07 ***\nJobLevel                                  0.000933 ***\nMonthlyIncome                             0.001472 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.1 on 23477 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.00395,   Adjusted R-squared:  0.003526 \nF-statistic:  9.31 on 10 and 23477 DF,  p-value: 1.403e-15\n\n\n\n\n\nLogistic Regression\nLogistic regression is used when the dependent variable is binary. We transform Gender into a binary variable and predict it based on MonthlyIncome and JobLevel.\n\n# Convert Gender to binary variable (Male = 1, Female = 0)\ndf &lt;- df %&gt;%\n  mutate(Gender = ifelse(Gender == \"Male\", 1, 0))\n\n# Logistic regression model\nlogit &lt;- glm(Gender ~ MonthlyIncome + JobLevel, data = df, family = \"binomial\")\n\n# Display the summary of the logistic regression model\nsummary(logit)\n\n\nCall:\nglm(formula = Gender ~ MonthlyIncome + JobLevel, family = \"binomial\", \n    data = df)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    5.965e-01  3.214e-02  18.558  &lt; 2e-16 ***\nMonthlyIncome  2.086e-05  8.476e-06   2.461   0.0138 *  \nJobLevel      -1.575e-01  3.611e-02  -4.363 1.28e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 31633  on 23503  degrees of freedom\nResidual deviance: 31589  on 23501  degrees of freedom\n  (16 observations deleted due to missingness)\nAIC: 31595\n\nNumber of Fisher Scoring iterations: 4\n\n# Exponentiate the coefficients to get odds ratios\nexp(coef(logit))\n\n  (Intercept) MonthlyIncome      JobLevel \n    1.8157319     1.0000209     0.8542492"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-cleaning",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-1-cleaning",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "üßπ Tutorial #1 (Cleaning)",
    "text": "üßπ Tutorial #1 (Cleaning)\n\nGetting Started\nBefore diving into the transformations, let‚Äôs load the necessary package and dataset. You can download the dataset to follow along.\n\n# Uncomment the next four lines if you haven't installed the package\n# install.packages(\"tidyverse\")\n# install.packages(\"psych\")\n# install.packages(\"lsr\")\n# install.packages(\"apaTables\")\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(lsr)\nlibrary(apaTables)\n\n# Loading the dataset\ndf &lt;- read_csv(\"IBM HR Data.csv\")\n\n# Checking all the column names\nnames(df)\n\n [1] \"Age\"                      \"Attrition\"               \n [3] \"BusinessTravel\"           \"DailyRate\"               \n [5] \"Department\"               \"DistanceFromHome\"        \n [7] \"Education\"                \"EducationField\"          \n [9] \"EmployeeCount\"            \"EmployeeNumber\"          \n[11] \"Application ID\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n[37] \"Employee Source\"         \n\n\n\n\n\nSorting Data\nSorting is often one of the first transformations you‚Äôll perform. Here‚Äôs how to do it in base R and tidyverse.\n\nOption 1: Base R\nIn base R, you can use the order() function to sort by one or more columns:\n\n# Sort by age in ascending order\nindex &lt;- order(df$Age) \ndf &lt;- df[index, ]\n\n\n\nOption 2: Tidyverse\nThe arrange() function from dplyr is a more concise and readable option:\n\n# Sort by multiple columns: Age (descending) and DailyRate (ascending)\ndf &lt;- arrange(df, desc(Age), DailyRate)\n\n\n\n\n\nRenaming Columns\nRenaming columns is another frequent task, especially when preparing data for analysis or visualization.\n\nOption 1: Base R\nUse the colnames() function to rename specific columns:\n\n# Rename the first column (age)\ncolnames(df)[1] &lt;- \"how_old\"\n\n\n\nOption 2: Tidyverse\nThe rename() function provides an intuitive way to rename columns:\n\n# Rename \"how_old\" back to \"age\"\ndf &lt;- rename(df, \"age\" = \"how_old\")\n\n\n\n\n\nFiltering Data\nFiltering rows based on specific conditions is crucial for focusing your analysis. Let‚Äôs filter for single employees earning over $10,000 per month:\n\n# Filter rows where MaritalStatus is \"Single\" and MonthlyIncome &gt; 10000\nsingle_rich &lt;- df %&gt;%\n  filter(MaritalStatus == \"Single\", MonthlyIncome &gt; 10000)\n\nThe pipe operator (%&gt;%) from the tidyverse makes it easy to chain multiple operations.\n\n\n\nSummarizing Data\nSummarization provides insights into key metrics like mean and standard deviation. Here‚Äôs how to summarize the filtered data:\n\n# summarizing the subset data\nsingle_rich %&gt;%\n  summarise(Mean = mean(MonthlyIncome), SD = sd(MonthlyIncome))\n\n# A tibble: 1 √ó 2\n    Mean    SD\n   &lt;dbl&gt; &lt;dbl&gt;\n1 15043. 3200.\n\n\nThis calculates the average (Mean) and standard deviation (SD) of the MonthlyIncome column for the filtered subset.\n\n\n\nSubsetting Columns\nSelecting specific columns is often necessary to focus on relevant data or remove unnecessary variables. You can use the select() function from the tidyverse to keep only the desired columns:\n\n# Keep specific columns\ndf %&gt;%\n  select(Attrition, DailyRate, Department, Education)\n\n# A tibble: 23,532 √ó 4\n   Attrition        DailyRate Department             Education\n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 Current employee       370 Research & Development         4\n 2 Current employee       370 Research & Development         4\n 3 Current employee       370 Research & Development         4\n 4 Current employee       370 Research & Development         4\n 5 Current employee       370 Research & Development         4\n 6 Current employee       370 Research & Development         4\n 7 Current employee       370 Research & Development         4\n 8 Current employee       370 Research & Development         4\n 9 Current employee       370 Research & Development         4\n10 Current employee       370 Research & Development         4\n# ‚Ñπ 23,522 more rows\n\n\nThis selects only the Attrition, DailyRate, Department, and Education columns.\nTo drop specific columns, use the select() function with the negation operator !:\n\n# Drop unnecessary columns\ndf &lt;- df %&gt;%\n  select(!c(EmployeeCount, StandardHours, Over18))\n\nThis removes the EmployeeCount, StandardHours, and Over18 columns.\n\n\nGrouping Data by Variable\nGrouping rows is helpful for summarizing data by categories. Here, we‚Äôll group the data by EducationField and count the number of occurrences in each category.\n\n# Group by EducationField and count rows\ndf %&gt;%\n  group_by(EducationField) %&gt;%\n  count() %&gt;%\n  arrange(desc(n))\n\n# A tibble: 9 √ó 2\n# Groups:   EducationField [9]\n  EducationField       n\n  &lt;chr&gt;            &lt;int&gt;\n1 Life Sciences     9725\n2 Medical           7393\n3 Marketing         2541\n4 Technical Degree  2105\n5 Other             1311\n6 Human Resources    446\n7 &lt;NA&gt;                 9\n8 3                    1\n9 Test                 1\n\n\nThis code:\n\nGroups the data by EducationField.\nCounts the rows in each group.\nArranges the groups in descending order by count (n).\n\n\n\nRecoding Variables\nRecoding variables is a common task when transforming categorical data into numerical or more meaningful labels. First, let‚Äôs check the frequency of each category in the Attrition column:\n\n# View the distribution of Attrition\ntable(df$Attrition)\n\n\n     Current employee           Termination Voluntary Resignation \n                19714                    96                  3709 \n\n\nYou can use the recode() function from the tidyverse to assign new values to the categories:\n\n# Recode Attrition variable\ndf$Attrition &lt;- recode(df$Attrition,\n                       \"Current employee\" = 0,\n                       \"Termination\" = 1,\n                       \"Voluntary Resignation\" = 2)\n\nThis transforms Attrition into a numerical variable with the following mapping:\n\n0: Current employee\n1: Termination\n2: Voluntary Resignation"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visualization",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visualization",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "Tutorial #2 (Visualization)",
    "text": "Tutorial #2 (Visualization)\nWe will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere‚Äôs how to create a simple scatterplot of YearsAtCompany vs.¬†TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we‚Äôll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere‚Äôs what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-4-reports",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-4-reports",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "üìù Tutorial #4 (Reports)",
    "text": "üìù Tutorial #4 (Reports)\nWe will use the apaTables package to format analysis outputs according to APA guidelines and export them to Word documents.\n\nAPA Style Correlation Table\nFirst, we create a correlation matrix using the cor() function for a subset of numeric columns from the dataset.\n\n# Subset of numeric columns for correlation analysis\nnum &lt;- df[, c(\"age\", \"DailyRate\", \"DistanceFromHome\", \n              \"HourlyRate\", \"MonthlyIncome\")]\n\n# Calculate the Pearson correlation matrix\ncorr_matrix &lt;- cor(num, method = \"pearson\", use = \"complete.obs\")\n\nNext, we use the apa.cor.table() function from the apaTables package to create a formatted APA-style table.\n\n# Create and save the APA correlation table\napa.cor.table(corr_matrix, table.number = 1, filename = \"Correlation_table.doc\")\n\n\n\nTable 1 \n\nMeans, standard deviations, and correlations with confidence intervals\n \n\n  Variable            M    SD   1           2           3           4          \n  1. age              0.25 0.43                                                \n                                                                               \n  2. DailyRate        0.20 0.45 -.30                                           \n                                [-.94, .79]                                    \n                                                                               \n  3. DistanceFromHome 0.20 0.45 -.33        -.27                               \n                                [-.94, .78] [-.93, .80]                        \n                                                                               \n  4. HourlyRate       0.20 0.44 -.35        -.20        -.21                   \n                                [-.94, .77] [-.92, .83] [-.92, .83]            \n                                                                               \n  5. MonthlyIncome    0.24 0.44 .24         -.35        -.32        -.35       \n                                [-.81, .93] [-.94, .77] [-.94, .78] [-.94, .77]\n                                                                               \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\nThe confidence interval is a plausible range of population correlations \nthat could have caused the sample correlation (Cumming, 2014).\n * indicates p &lt; .05. ** indicates p &lt; .01.\n \n\n\nThe above code will generate a Word document with the correlation table in APA format, and the file will be saved as ‚ÄúCorrelation_table.doc‚Äù.\n\n\n\nAPA Style ANOVA Table\nWe will use the apa.aov.table() function to format the ANOVA results into an APA-style table and save it to a Word document.\n\n# Create and save the APA ANOVA table\napa.aov.table(anovaTest, table.number = 2, filename = \"Anova_table.doc\")\n\n\n\nTable 2 \n\nANOVA results using df$JobSatisfaction as the dependent variable\n \n\n     Predictor       SS    df      MS       F    p partial_eta2\n   (Intercept)  7439.59     1 7439.59 6128.95 .000             \n df$Department     0.73     2    0.36    0.30 .742          .00\n         Error 28524.14 23499    1.21                          \n CI_90_partial_eta2\n                   \n         [.00, .00]\n                   \n\nNote: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared \n\n\nThis will generate a Word document containing the APA-formatted ANOVA table, saved as ‚ÄúAnova_table.doc‚Äù."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#conclusion",
    "href": "posts/2024-12-31-R-Workshop/index.html#conclusion",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "üí≠ Conclusion",
    "text": "üí≠ Conclusion\nIn this blog post, we covered key techniques for data manipulation, analysis, and reporting in R. We started with basic operations such as sorting, filtering, and renaming columns using both base R and the dplyr package. We then explored advanced visualization with ggplot2, creating customized scatterplots and faceted plots. The analysis section demonstrated t-tests, ANOVA, regression models, and post-hoc tests to analyze relationships in the data.\nWe also learned how to create APA-style tables for correlation matrices and ANOVA results using the apaTables package. While we‚Äôve only scratched the surface of what we can do with R, these tutorials should provide a robust foundation for conducting data analysis in research, business, and academic contexts, while hopefully mitigating initial anxieties associated with learning to code. Happy coding!"
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visuals",
    "href": "posts/2024-12-31-R-Workshop/index.html#tutorial-2-visuals",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "üé® Tutorial #2 (Visuals)",
    "text": "üé® Tutorial #2 (Visuals)\nWe will use ggplot2 to create visually appealing scatterplots, apply faceting for multi-panel plots, and customize plots for better presentation. These techniques are essential for exploratory data analysis and communicating insights effectively.\n\nScatterplots in ggplot2\nScatterplots are a powerful way to visualize relationships between two continuous variables.\nHere‚Äôs how to create a simple scatterplot of YearsAtCompany vs.¬†TotalWorkingYears with points colored red:\n\n# Basic scatterplot\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears), color = \"Red\")\n\n\n\n\nThis code plots YearsAtCompany on the x-axis and TotalWorkingYears on the y-axis with red points.\n\nAdding Aesthetic Mappings\nTo add another dimension, such as coloring points by MaritalStatus, use the aes() function:\n\n# Scatterplot with points colored by MaritalStatus\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = TotalWorkingYears, color = MaritalStatus))\n\n\n\n\n\n\n\n\nFaceting for Multi-Panel Plots\nFaceting allows you to split the data into panels based on a categorical variable.\n\n# Faceted scatterplot by JobLevel\nggplot(data = df) +\n  geom_point(aes(x = YearsAtCompany, y = YearsInCurrentRole)) +\n  facet_wrap(~JobLevel, nrow = 2)\n\n\n\n\nThis creates separate scatterplots for each level of JobLevel, arranged in two rows.\n\n\n\nMore Cusomization\nFor this section, we‚Äôll use the built-in mpg dataset. To enhance a scatterplot, you can map multiple aesthetics (e.g., color and size), customize axis labels, and add a title:\n\n# Load the dataset\ndata(mpg)\n\n# Scatterplot with multiple aesthetics and customizations\nggplot(mpg, aes(x = displ, y = hwy, color = drv, size = cty)) +\n  geom_point(alpha = 0.7) + # Add points with transparency\n  # Customize axis labels and plot titles\n  labs(title = \"Fuel Efficiency vs. Engine Displacement\",\n       x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       color = \"Drive Type\",\n       size = \"City MPG\") +\n  # Apply a theme\n  theme_bw() +\n  facet_wrap(~class)\n\n\n\n\nHere‚Äôs what this code does:\n\nAesthetics: Maps drv (drive type) to color and cty (city MPG) to point size.\nLabels: Adds a title, and custom axis labels, and modifies the legend titles.\nTheme: Applies a clean, white background theme (theme_bw()).\nFaceting: Creates facets for each vehicle class."
  },
  {
    "objectID": "posts/2024-12-31-R-Workshop/index.html#reference",
    "href": "posts/2024-12-31-R-Workshop/index.html#reference",
    "title": "R You Ready? An I/O Psychologist‚Äôs Guide to R & Rstudio",
    "section": "üîó Reference",
    "text": "üîó Reference\nWickham, H., & Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O‚ÄôReilly Media."
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html",
    "href": "posts/2024-05-21-LLM-BERT/index.html",
    "title": "From Text to Predictions: Using LLMs for NLP Tasks",
    "section": "",
    "text": "We start by importing a pre-trained LLM (BERTBASE) and tokenizer using the AutoModel and BertTokenizer from the transformers library.\n# Import the necesary modules\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, cross_val_predict, cross_val_score\nfrom scipy.stats import pearsonr\nfrom transformers import AutoModel, BertTokenizer\n\n# Import the LLM and tokenizer\nbert = AutoModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nFor those interested in exploring other Large Language Models (LLMs), the Hugging Face Transformers library offers easy access to a variety of pre-trained models, including google/flan-t5-large, Falcon3-10B-Base, albert-base-v2, roberta-base, distilbert-base-uncased. It‚Äôs crucial, however, to verify that the chosen LLM is capable of the specific natural language tasks you intend to perform (e.g., tokenization, classification, summarization). Check out the Hugging Face documentation for more details."
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html#importing-llm-tokenizer",
    "href": "posts/2024-05-21-LLM-BERT/index.html#importing-llm-tokenizer",
    "title": "From Text to Predictions: Using LLMs for NLP Tasks",
    "section": "",
    "text": "We start by importing a pre-trained LLM (BERTBASE) and tokenizer using the AutoModel and BertTokenizer from the transformers library.\n# Import the necesary modules\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, cross_val_predict, cross_val_score\nfrom scipy.stats import pearsonr\nfrom transformers import AutoModel, BertTokenizer\n\n# Import the LLM and tokenizer\nbert = AutoModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nFor those interested in exploring other Large Language Models (LLMs), the Hugging Face Transformers library offers easy access to a variety of pre-trained models, including google/flan-t5-large, Falcon3-10B-Base, albert-base-v2, roberta-base, distilbert-base-uncased. It‚Äôs crucial, however, to verify that the chosen LLM is capable of the specific natural language tasks you intend to perform (e.g., tokenization, classification, summarization). Check out the Hugging Face documentation for more details."
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html#text-to-text-embeddings",
    "href": "posts/2024-05-21-LLM-BERT/index.html#text-to-text-embeddings",
    "title": "Leveraging LLMs for NLP: Tokenization, Embeddings, and Training",
    "section": "Text to Text-embeddings",
    "text": "Text to Text-embeddings\nText embeddings are a fundamental NLP technique that transforms textual data into numerical vectors, making it digestible for processing by Machine Learning (ML) models (Ma et al., 2019). These embeddings and their large dimensions encapsulate the semantic, syntactic, and contextual properties of the text in a format that can be processed efficiently by computers.\n\nTokenization\nThe first step to transform the texts into text-embeddings is tokenization. Tokenization, a fundamental preprocessing step in natural language processing (NLP), involves segmenting text into smaller units called tokens (Bird et al., 2009). Tokens are essentially segments of the text that are treated as distinct units for analysis, typically words or subwords. For example, word-level tokenization would split ‚ÄúNatural language processing is fascinating‚Äù into [‚ÄúNatural‚Äù, ‚Äúlanguage‚Äù, ‚Äúprocessing‚Äù, ‚Äúis‚Äù, ‚Äúfascinating‚Äù]. Modern tokenization methods typically utilize subwords due to their increased utility.\nBefore tokenization, establishing a consistent token size across all data points is beneficial. This standardization allows LLMs to process text and generate embeddings more efficiently. I prefer setting the token size to accommodate the longest responses to avoid data loss, with shorter responses padded to match. Alternatively, using the average token length is possible, but this approach risks truncating longer responses and losing data.\n# Calculate the length of each text entry in tokens\ntoken_lengths = []  # List to store token counts\nfor text in df['text_variable']: # change 'text_variable' to the variable in your dataset\n    token_lengths.append(len(text.split()))\n\nmax_token = max(token_lengths)\nmin_token = min(token_lengths)\n\n# Print the shortest and longest tokenized responses\nprint(\"Shortest response (in tokens):\", max_token)\nprint(\"Longest response (in tokens):\", max_token)\n\n\nText-embeddings\nAfter tokenization, the resulting tokens are fed into a pre-trained model (e.g., BERTBASE) to generate text embeddings. These embeddings are high-dimensional vectors; for BERTBASE, the output is a (sample_size, 768) matrix, while for BERTLARGE, it‚Äôs (sample_size, 1024). Several strategies exist for utilizing these outputs. Embeddings can be extracted from a single layer (out of 12 for BERTBASE and 24 for BERTLARGE) or by concatenating or averaging embeddings from multiple layers. Extracting from the last layer, the second-to-last layer, and averaging or concatenating the last four layers has proven particularly effective for various NLP tasks (Devlin et al., 2018; Kjell et al., 2023). The code snippet below provides an example of a function that generates text embeddings by averaging the output of the last four hidden layers.\n# Function for text-embeddings (avg of last four hidden layers)\ndef mean_of_last_four_layers(text):\n    \"\"\"Encode text and compute the mean of the embeddings from the last four BERT layers.\"\"\"\n    # Tokenize text and convert to tensor\n    encoded_input = tokenizer(text, max_length=max_token, padding='max_length', truncation=True, return_tensors='pt')\n\n    # Forward pass, disable gradient calculation\n    with torch.no_grad():\n        outputs = bert(**encoded_input)\n        hidden_states = outputs.hidden_states\n\n    # Stack the last four layers and calculate the mean along the new dimension (0)\n    last_four_layers = torch.stack((hidden_states[-4], hidden_states[-3], hidden_states[-2], hidden_states[-1]))\n    mean_embeddings = torch.mean(last_four_layers, dim=0)\n\n    return mean_embeddings.squeeze().numpy()\n\n# Apply the function to the 'text_variable' column\ndf['encoded'] = df['text_variable'].apply(lambda x: mean_of_last_four_layers(x))"
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html#references",
    "href": "posts/2024-05-21-LLM-BERT/index.html#references",
    "title": "From Text to Predictions: Using LLMs for NLP Tasks",
    "section": "üîó References",
    "text": "üîó References\nBiggiogera, J., Boateng, G., Hilpert, P., Vowels, M., Bodenmann, G., Neysari, M., ‚Ä¶ & Kowatsch, T. (2021, October). BERT meets LIWC: Exploring state-of-the-art language models for predicting communication behavior in couples‚Äô conflict interactions. In Companion Publication of the 2021 International Conference on Multimodal Interaction (pp.¬†385-389). https://doi.org/10.1145/3461615.3485423\nBird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. ‚Äù O‚ÄôReilly Media, Inc.‚Äù.\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. https://doi.org/10.48550/arXiv.1810.04805\nKjell, O. N., Kjell, K., & Schwartz, H. A. (2023). Beyond rating scales: With targeted evaluation, language models are poised for psychological assessment. Psychiatry Research, 115667. https://doi.org/10.1016/j.psychres.2023.115667\nMa, X., Wang, Z., Ng, P., Nallapati, R., & Xiang, B. (2019). Universal text representation from bert: An empirical study. arXiv preprint arXiv:1910.07973. https://doi.org/10.48550/arXiv.1910.07973\nMaini, P., Kolluru, K., & Pruthi, D. (2020). Why and when should you pool? Analyzing Pooling in Recurrent Architectures. arXiv preprint arXiv:2005.00159."
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html#generating-text-embeddings-from-texts",
    "href": "posts/2024-05-21-LLM-BERT/index.html#generating-text-embeddings-from-texts",
    "title": "From Text to Predictions: Using LLMs for NLP Tasks",
    "section": "üß† Generating Text-embeddings From Texts",
    "text": "üß† Generating Text-embeddings From Texts\nText embeddings are a fundamental NLP technique that transforms textual data into numerical vectors, making it digestible for processing by Machine Learning (ML) models (Ma et al., 2019). These embeddings and their large dimensions encapsulate the semantic, syntactic, and contextual properties of the text in a format that can be processed efficiently by computers.\n\nTokenization\nThe first step to transform the texts into text-embeddings is tokenization. Tokenization, a fundamental preprocessing step in natural language processing (NLP), involves segmenting text into smaller units called tokens (Bird et al., 2009). Tokens are essentially segments of the text that are treated as distinct units for analysis, typically words or subwords. For example, word-level tokenization would split ‚ÄúNatural language processing is fascinating‚Äù into [‚ÄúNatural‚Äù, ‚Äúlanguage‚Äù, ‚Äúprocessing‚Äù, ‚Äúis‚Äù, ‚Äúfascinating‚Äù]. Modern tokenization methods typically utilize subwords due to their increased utility.\nBefore tokenization, establishing a consistent token size across all data points is beneficial. This standardization allows LLMs to process text and generate embeddings more efficiently. I prefer setting the token size to accommodate the longest responses to avoid data loss, with shorter responses padded to match. Alternatively, using the average token length is possible, but this approach risks truncating longer responses and losing data.\n# Calculate the length of each text entry in tokens\ntoken_lengths = []  # List to store token counts\nfor text in df['text_variable']: # change 'text_variable' to the variable in your dataset\n    token_lengths.append(len(text.split()))\n\nmax_token = max(token_lengths)\nmin_token = min(token_lengths)\n\n# Print the shortest and longest tokenized responses\nprint(\"Shortest response (in tokens):\", max_token)\nprint(\"Longest response (in tokens):\", max_token)\n\n\nText-embeddings\nAfter tokenization, the resulting tokens are fed into a pre-trained model (e.g., BERTBASE) to generate text embeddings. These embeddings are high-dimensional vectors; for BERTBASE, the output is a (sample_size, 768) matrix, while for BERTLARGE, it‚Äôs (sample_size, 1024). Several strategies exist for utilizing these outputs. Embeddings can be extracted from a single layer (out of 12 for BERTBASE and 24 for BERTLARGE) or by concatenating or averaging embeddings from multiple layers. Extracting from the last layer, the second-to-last layer, and averaging or concatenating the last four layers has proven particularly effective for various NLP tasks (Devlin et al., 2018; Kjell et al., 2023). The code snippet below provides an example of a function that generates text embeddings by averaging the output of the last four hidden layers.\n# Function for text-embeddings that encode the average of last four hidden layers\ndef mean_of_last_four_layers(text):\n\n    # Tokenize text and convert to tensor\n    encoded_input = tokenizer(text, max_length=max_token, padding='max_length', truncation=True, return_tensors='pt')\n\n    # Forward pass, disable gradient calculation\n    with torch.no_grad():\n        outputs = bert(**encoded_input)\n        hidden_states = outputs.hidden_states\n\n    # Stack the last four layers and calculate the mean along the new dimension (0)\n    last_four_layers = torch.stack((hidden_states[-4], hidden_states[-3], hidden_states[-2], hidden_states[-1]))\n    mean_embeddings = torch.mean(last_four_layers, dim=0)\n\n    return mean_embeddings.squeeze().numpy()\n\n# Apply the function to the 'text_variable' column\ndf['encoded'] = df['text_variable'].apply(lambda x: mean_of_last_four_layers(x))\nAs an alternative to averaging the last four hidden layers, you can extract the output from a specific layer. For instance, to use the second-to-last layer (layer 11 in BERTBASE), use the following code:\n# Function for embeddings from a specific BERT layer\ndef encode_text(text, layer_index):\n\n    # Tokenize text and convert to tensor\n    encoded_input = tokenizer(text, max_length = max_token, padding = 'max_length', truncation=True, return_tensors='pt')\n\n    # Forward pass, disable gradient calculation\n    with torch.no_grad():\n        outputs = bert(**encoded_input)\n        hidden_states = outputs.hidden_states\n\n    # Select the embeddings from the specified layer\n    return hidden_states[layer_index].squeeze().numpy()\n\n# 11th layer (second to last)\n# Change this to get embeddings from a different layer\nselected_layer = 11\n\n# Encode the column (this example directly updates the DataFrame)\ndf['encoded_layer_11'] = df['text_variable'].apply(lambda x: encode_text(x, layer_index=selected_layer))"
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html#fitting-text-embeddings-into-ml-models",
    "href": "posts/2024-05-21-LLM-BERT/index.html#fitting-text-embeddings-into-ml-models",
    "title": "From Text to Predictions: Using LLMs for NLP Tasks",
    "section": "ü§ñ Fitting Text-embeddings into ML Models",
    "text": "ü§ñ Fitting Text-embeddings into ML Models\nTo use text embeddings in regression or machine learning models, you can perform mean pooling, reducing the three-dimensional token embeddings (sample_size, token_size, 768) to two-dimensional vectors (sample_size, 768). This process averages the embeddings across all tokens per document/data point, creating a single vector representing the overall semantic content of individual responses. While mean pooling might lose some contextual nuances, it balances detail with computational efficiency, effectively filtering noise and ensuring no single token overly influences the model (Biggiogera et al., 2021; Ma et al., 2019; Maini et al., 2020). You will then use these mean embeddings as input features for different ML models.\n# Splitting the dataset into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Converting the 'encoded' column to a numpy array and averaging across tokens\ntrain_embeddings = np.array(train_df['encoded'].tolist())\nx_train = np.mean(train_embeddings, axis=1)\n\ntest_embeddings = np.array(test_df['encoded'].tolist())\nx_test = np.mean(test_embeddings, axis=1)\n\n# Converting the target variable column to a numpy array\ny_train = np.array(train_df['target_variable'].tolist())\ny_test = np.array(test_df['target_variable'].tolist())\n\n# Initializing the regression model\nrf_regressor = RandomForestRegressor(random_state=42) # to use a different ML model change this line\n\n# Training the model\nrf_regressor.fit(x_train, y_train)\n\n# Making predictions on the test set\ntest_predictions = rf_regressor.predict(x_test)\n\n# Evaluating the model\nmse = mean_squared_error(y_test, test_predictions)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, test_predictions)\ncor = np.corrcoef(y_test, test_predictions)\n\n# Displaying evaluation metrics\nprint(f'Mean Squared Error (MSE): {mse:.4f}')\nprint(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\nprint(f'R-squared (R¬≤): {r2:.4f}')\nprint(f'Correlation Coefficient: {cor[0, 1]:.4f}')\n\nK-fold Cross-validation\n# Convert the 'encoded' column to a numpy array and average across tokens\nembeddings = np.array(df['encoded'].tolist())\nx_input = np.mean(embeddings, axis=1)\n\n# Convert the target variable column to a numpy array\ninput_y = np.array(df['target_variable'].tolist())  # Replace 'target_variable' with the actual column name\n\n# Initialize the Random Forest regressor\nregressor = RandomForestRegressor(random_state=42)\n\n# K-Fold Cross-Validation setup\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Cross-validation: Calculate Negative Mean Squared Error (converted to RMSE)\nmse_scores = cross_val_score(regressor, x_input, input_y, cv=kfold, scoring='neg_mean_squared_error')\nmean_rmse = np.mean(np.sqrt(-mse_scores))\nprint(f'Mean Root Mean Squared Error (RMSE): {mean_rmse:.4f}')\n\n# Cross-validation: Calculate R-squared (R¬≤) scores\nr2_scores = cross_val_score(regressor, x_input, input_y, cv=kfold, scoring='r2')\nmean_r2 = np.mean(r2_scores)\nprint(f'Mean R-squared (R¬≤): {mean_r2:.4f}')\n\n# Cross-validation: Generate predictions for each fold\npredictions = cross_val_predict(regressor, x_input, input_y, cv=kfold)\n\n# Calculate the Pearson correlation coefficient\ncorrelation, _ = pearsonr(input_y, predictions)\nprint(f'Correlation Coefficient: {correlation:.4f}')"
  },
  {
    "objectID": "posts/2024-05-21-LLM-BERT/index.html#conclusion",
    "href": "posts/2024-05-21-LLM-BERT/index.html#conclusion",
    "title": "From Text to Predictions: Using LLMs for NLP Tasks",
    "section": "üí≠ Conclusion",
    "text": "üí≠ Conclusion\nIn this blog post, I demonstrated how to leverage pre-trained LLMs like BERT for NLP tasks, covering text tokenization, embedding extraction, and their application in predictive ML models. We explored techniques such as averaging or selecting embeddings from specific hidden layers to balance computational efficiency and contextual richness. We also showed how to integrate these embeddings into ML pipelines, including training, prediction and evaluation using RMSE, R¬≤, and k-fold cross-validation. Using LLM-based embeddings for feature engineering allows models to capture nuanced textual relationships, empowering solutions for text classification, sentiment analysis, and other NLP challenges.\nI hope this post provided a helpful starting point for using LLMs to analyze large amounts of unstructured text data for prediction. With further exploration, you‚Äôll be ready to tackle more advanced techniques, including using regression heads directly on LLMs, fine-tuning models for specific tasks, and utilizing LLMs pre-trained on specialized corpora.Feel free to connect and share your progress and research."
  }
]